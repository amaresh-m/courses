question_num,search_query,url,Question,Options,Correct answer,Image URLs,Comments
51,examtopics databricks data engineer professional certification question 51,https://www.examtopics.com/discussions/databricks/view/133247-exam-certified-data-engineer-professional-topic-1-question/,Where in the Spark UI can one diagnose a performance problem induced by not leveraging predicate push-down?,"['A.In the Executor’s log file, by grepping for ""predicate push-down""', 'B.In the Stage’s Detail screen, in the Completed Stages table, by noting the size of data read from the Input column', 'C.In the Storage Detail screen, by noting which RDDs are not stored on disk', 'D.In the Delta Lake transaction log. by noting the column statistics', 'E.In the Query Detail screen, by interpreting the Physical Plan']","['E.In the Query Detail screen, by interpreting the Physical Plan']",[],
52,examtopics databricks data engineer professional certification question 52,https://www.examtopics.com/discussions/databricks/view/120580-exam-certified-data-engineer-professional-topic-1-question/,Review the following error traceback:Which statement describes the error being raised?,"['A.The code executed was PySpark but was executed in a Scala notebook.', 'B.There is no column in the table named heartrateheartrateheartrate', 'C.There is a type error because a column object cannot be multiplied.', 'D.There is a type error because a DataFrame object cannot be multiplied.', 'E.There is a syntax error because the heartrate column is not correctly identified as a column.']",['B.There is no column in the table named heartrateheartrateheartrate'],['https://img.examtopics.com/certified-data-engineer-professional/image26.png'],
53,examtopics databricks data engineer professional certification question 53,https://www.examtopics.com/discussions/databricks/view/128879-exam-certified-data-engineer-professional-topic-1-question/,Which distribution does Databricks support for installing custom Python code packages?,"['A.sbt', 'B.CRANC. npm', 'D.Wheels', 'E.jars']",['D.Wheels'],[],
54,examtopics databricks data engineer professional certification question 54,https://www.examtopics.com/discussions/databricks/view/128880-exam-certified-data-engineer-professional-topic-1-question/,Which Python variable contains a list of directories to be searched when trying to locate required modules?,"['A.importlib.resource_path', 'B.sys.path', 'C.os.path', 'D.pypi.path', 'E.pylib.source']",['B.sys.path'],[],
55,examtopics databricks data engineer professional certification question 55,https://www.examtopics.com/discussions/databricks/view/128881-exam-certified-data-engineer-professional-topic-1-question/,"Incorporating unit tests into a PySpark application requires upfront attention to the design of your jobs, or a potentially significant refactoring of existing code.Which statement describes a main benefit that offset this additional effort?","['A.Improves the quality of your data', 'B.Validates a complete use case of your application', 'C.Troubleshooting is easier since all steps are isolated and tested individually', 'D.Yields faster deployment and execution times', 'E.Ensures that all steps interact correctly to achieve the desired end result']",['C.Troubleshooting is easier since all steps are isolated and tested individually'],[],
56,examtopics databricks data engineer professional certification question 56,https://www.examtopics.com/discussions/databricks/view/128882-exam-certified-data-engineer-professional-topic-1-question/,Which statement describes integration testing?,"['A.Validates interactions between subsystems of your application', 'B.Requires an automated testing framework', 'C.Requires manual intervention', 'D.Validates an application use case', 'E.Validates behavior of individual elements of your application']",['A.Validates interactions between subsystems of your application'],[],
57,examtopics databricks data engineer professional certification question 57,https://www.examtopics.com/discussions/databricks/view/127546-exam-certified-data-engineer-professional-topic-1-question/,Which REST API call can be used to review the notebooks configured to run as tasks in a multi-task job?,"['A./jobs/runs/list', 'B./jobs/runs/get-output', 'C./jobs/runs/get', 'D./jobs/get', 'E./jobs/list']",['D./jobs/get'],[],
58,examtopics databricks data engineer professional certification question 58,https://www.examtopics.com/discussions/databricks/view/118942-exam-certified-data-engineer-professional-topic-1-question/,"A Databricks job has been configured with 3 tasks, each of which is a Databricks notebook. Task A does not depend on other tasks. Tasks B and C run in parallel, with each having a serial dependency on task A.If tasks A and B complete successfully but task C fails during a scheduled run, which statement describes the resulting state?","['A.All logic expressed in the notebook associated with tasks A and B will have been successfully completed; some operations in task C may have completed successfully.', 'B.All logic expressed in the notebook associated with tasks A and B will have been successfully completed; any changes made in task C will be rolled back due to task failure.', 'C.All logic expressed in the notebook associated with task A will have been successfully completed; tasks B and C will not commit any changes because of stage failure.', 'D.Because all tasks are managed as a dependency graph, no changes will be committed to the Lakehouse until ail tasks have successfully been completed.', 'E.Unless all tasks complete successfully, no changes will be committed to the Lakehouse; because task C failed, all commits will be rolled back automatically.']",['A.All logic expressed in the notebook associated with tasks A and B will have been successfully completed; some operations in task C may have completed successfully.'],[],
59,examtopics databricks data engineer professional certification question 59,https://www.examtopics.com/discussions/databricks/view/132441-exam-certified-data-engineer-professional-topic-1-question/,"A Delta Lake table was created with the below query:Realizing that the original query had a typographical error, the below code was executed:ALTER TABLE prod.sales_by_stor RENAME TO prod.sales_by_storeWhich result will occur after running the second command?","['A.The table reference in the metastore is updated and no data is changed.', 'B.The table name change is recorded in the Delta transaction log.', 'C.All related files and metadata are dropped and recreated in a single ACID transaction.', 'D.The table reference in the metastore is updated and all data files are moved.', 'E.A new Delta transaction log Is created for the renamed table.']",['A.The table reference in the metastore is updated and no data is changed.'],['https://img.examtopics.com/certified-data-engineer-professional/image27.png'],
60,examtopics databricks data engineer professional certification question 60,https://www.examtopics.com/discussions/databricks/view/120443-exam-certified-data-engineer-professional-topic-1-question/,"The data engineering team maintains a table of aggregate statistics through batch nightly updates. This includes total sales for the previous day alongside totals and averages for a variety of time periods including the 7 previous days, year-to-date, and quarter-to-date. This table is named store_saies_summary and the schema is as follows:The table daily_store_sales contains all the information needed to update store_sales_summary. The schema for this table is: store_id INT, sales_date DATE, total_sales FLOATIf daily_store_sales is implemented as a Type 1 table and the total_sales column might be adjusted after manual data auditing, which approach is the safest to generate accurate reports in the store_sales_summary table?","['A.Implement the appropriate aggregate logic as a batch read against the daily_store_sales table and overwrite the store_sales_summary table with each Update.', 'B.Implement the appropriate aggregate logic as a batch read against the daily_store_sales table and append new rows nightly to the store_sales_summary table.', 'C.Implement the appropriate aggregate logic as a batch read against the daily_store_sales table and use upsert logic to update results in the store_sales_summary table.', 'D.Implement the appropriate aggregate logic as a Structured Streaming read against the daily_store_sales table and use upsert logic to update results in the store_sales_summary table.', 'E.Use Structured Streaming to subscribe to the change data feed for daily_store_sales and apply changes to the aggregates in the store_sales_summary table with each update.']",['A.Implement the appropriate aggregate logic as a batch read against the daily_store_sales table and overwrite the store_sales_summary table with each Update.'],['https://img.examtopics.com/certified-data-engineer-professional/image28.png'],
61,examtopics databricks data engineer professional certification question 61,https://www.examtopics.com/discussions/databricks/view/125249-exam-certified-data-engineer-professional-topic-1-question/,A member of the data engineering team has submitted a short notebook that they wish to schedule as part of a larger data pipeline. Assume that the commands provided below produce the logically correct results when run as presented.Which command should be removed from the notebook before scheduling it as a job?,"['A.Cmd 2', 'B.Cmd 3', 'C.Cmd 4', 'D.Cmd 5', 'E.Cmd 6']",['E.Cmd 6'],['https://img.examtopics.com/certified-data-engineer-professional/image29.png'],
62,examtopics databricks data engineer professional certification question 62,https://www.examtopics.com/discussions/databricks/view/126360-exam-certified-data-engineer-professional-topic-1-question/,"The business reporting team requires that data for their dashboards be updated every hour. The total processing time for the pipeline that extracts transforms, and loads the data for their pipeline runs in 10 minutes.Assuming normal operating conditions, which configuration will meet their service-level agreement requirements with the lowest cost?","['A.Manually trigger a job anytime the business reporting team refreshes their dashboards', 'B.Schedule a job to execute the pipeline once an hour on a new job cluster', 'C.Schedule a Structured Streaming job with a trigger interval of 60 minutes', 'D.Schedule a job to execute the pipeline once an hour on a dedicated interactive cluster', 'E.Configure a job that executes every time new data lands in a given directory']",['B.Schedule a job to execute the pipeline once an hour on a new job cluster'],[],
63,examtopics databricks data engineer professional certification question 63,https://www.examtopics.com/discussions/databricks/view/124566-exam-certified-data-engineer-professional-topic-1-question/,A Databricks SQL dashboard has been configured to monitor the total number of records present in a collection of Delta Lake tables using the following query pattern:SELECT COUNT (*) FROM table -Which of the following describes how results are generated each time the dashboard is updated?,"['A.The total count of rows is calculated by scanning all data files', 'B.The total count of rows will be returned from cached results unless REFRESH is run', 'C.The total count of records is calculated from the Delta transaction logs', 'D.The total count of records is calculated from the parquet file metadata', 'E.The total count of records is calculated from the Hive metastore']",['C.The total count of records is calculated from the Delta transaction logs'],[],
64,examtopics databricks data engineer professional certification question 64,https://www.examtopics.com/discussions/databricks/view/124117-exam-certified-data-engineer-professional-topic-1-question/,"A Delta Lake table was created with the below query:Consider the following query:DROP TABLE prod.sales_by_store -If this statement is executed by a workspace admin, which result will occur?","['A.Nothing will occur until a COMMIT command is executed.', 'B.The table will be removed from the catalog but the data will remain in storage.', 'C.The table will be removed from the catalog and the data will be deleted.', 'D.An error will occur because Delta Lake prevents the deletion of production data.', 'E.Data will be marked as deleted but still recoverable with Time Travel.']",['C.The table will be removed from the catalog and the data will be deleted.'],['https://img.examtopics.com/certified-data-engineer-professional/image30.png'],
65,examtopics databricks data engineer professional certification question 65,https://www.examtopics.com/discussions/databricks/view/124569-exam-certified-data-engineer-professional-topic-1-question/,Two of the most common data locations on Databricks are the DBFS root storage and external object storage mounted with dbutils.fs.mount().Which of the following statements is correct?,"['A.DBFS is a file system protocol that allows users to interact with files stored in object storage using syntax and guarantees similar to Unix file systems.', 'B.By default, both the DBFS root and mounted data sources are only accessible to workspace administrators.', 'C.The DBFS root is the most secure location to store data, because mounted storage volumes must have full public read and write permissions.', 'D.Neither the DBFS root nor mounted storage can be accessed when using %sh in a Databricks notebook.', 'E.The DBFS root stores files in ephemeral block volumes attached to the driver, while mounted directories will always persist saved data to external storage between sessions.']",['A.DBFS is a file system protocol that allows users to interact with files stored in object storage using syntax and guarantees similar to Unix file systems.'],[],
66,examtopics databricks data engineer professional certification question 66,https://www.examtopics.com/discussions/databricks/view/124571-exam-certified-data-engineer-professional-topic-1-question/,"The following code has been migrated to a Databricks notebook from a legacy workload:The code executes successfully and provides the logically correct results, however, it takes over 20 minutes to extract and load around 1 GB of data.Which statement is a possible explanation for this behavior?","['A.%sh triggers a cluster restart to collect and install Git. Most of the latency is related to cluster startup time.', 'B.Instead of cloning, the code should use %sh pip install so that the Python code can get executed in parallel across all nodes in a cluster.', 'C.%sh does not distribute file moving operations; the final line of code should be updated to use %fs instead.', 'D.Python will always execute slower than Scala on Databricks. The run.py script should be refactored to Scala.', 'E.%sh executes shell code on the driver node. The code does not take advantage of the worker nodes or Databricks optimized Spark.']",['E.%sh executes shell code on the driver node. The code does not take advantage of the worker nodes or Databricks optimized Spark.'],['https://img.examtopics.com/certified-data-engineer-professional/image31.png'],
67,examtopics databricks data engineer professional certification question 67,https://www.examtopics.com/discussions/databricks/view/124572-exam-certified-data-engineer-professional-topic-1-question/,"The data science team has requested assistance in accelerating queries on free form text from user reviews. The data is currently stored in Parquet with the below schema:item_id INT, user_id INT, review_id INT, rating FLOAT, review STRINGThe review column contains the full text of the review left by the user. Specifically, the data science team is looking to identify if any of 30 key words exist in this field.A junior data engineer suggests converting this data to Delta Lake will improve query performance.Which response to the junior data engineer s suggestion is correct?","['A.Delta Lake statistics are not optimized for free text fields with high cardinality.', 'B.Text data cannot be stored with Delta Lake.', 'C.ZORDER ON review will need to be run to see performance gains.', 'D.The Delta log creates a term matrix for free text fields to support selective filtering.', 'E.Delta Lake statistics are only collected on the first 4 columns in a table.']",['A.Delta Lake statistics are not optimized for free text fields with high cardinality.'],[],
68,examtopics databricks data engineer professional certification question 68,https://www.examtopics.com/discussions/databricks/view/124574-exam-certified-data-engineer-professional-topic-1-question/,"Assuming that the Databricks CLI has been installed and configured correctly, which Databricks CLI command can be used to upload a custom Python Wheel to object storage mounted with the DBFS for use with a production job?","['A.configure', 'B.fs', 'C.jobs', 'D.libraries', 'E.workspace']",['B.fs'],[],
69,examtopics databricks data engineer professional certification question 69,https://www.examtopics.com/discussions/databricks/view/129697-exam-certified-data-engineer-professional-topic-1-question/,"The business intelligence team has a dashboard configured to track various summary metrics for retail stores. This includes total sales for the previous day alongside totals and averages for a variety of time periods. The fields required to populate this dashboard have the following schema:For demand forecasting, the Lakehouse contains a validated table of all itemized sales updated incrementally in near real-time. This table, named products_per_order, includes the following fields:Because reporting on long-term sales trends is less volatile, analysts using the new dashboard only require data to be refreshed once daily. Because the dashboard will be queried interactively by many users throughout a normal business day, it should return results quickly and reduce total compute associated with each materialization.Which solution meets the expectations of the end users while controlling and limiting possible costs?","['A.Populate the dashboard by configuring a nightly batch job to save the required values as a table overwritten with each update.', 'B.Use Structured Streaming to configure a live dashboard against the products_per_order table within a Databricks notebook.', 'C.Configure a webhook to execute an incremental read against products_per_order each time the dashboard is refreshed.', 'D.Use the Delta Cache to persist the products_per_order table in memory to quickly update the dashboard with each query.', 'E.Define a view against the products_per_order table and define the dashboard against this view.']",['A.Populate the dashboard by configuring a nightly batch job to save the required values as a table overwritten with each update.'],"['https://img.examtopics.com/certified-data-engineer-professional/image32.png', 'https://img.examtopics.com/certified-data-engineer-professional/image33.png']",
70,examtopics databricks data engineer professional certification question 70,https://www.examtopics.com/discussions/databricks/view/124575-exam-certified-data-engineer-professional-topic-1-question/,"A data ingestion task requires a one-TB JSON dataset to be written out to Parquet with a target part-file size of 512 MB. Because Parquet is being used instead of Delta Lake, built-in file-sizing features such as Auto-Optimize & Auto-Compaction cannot be used.Which strategy will yield the best performance without shuffling data?","['A.Set spark.sql.files.maxPartitionBytes to 512 MB, ingest the data, execute the narrow transformations, and then write to parquet.', 'B.Set spark.sql.shuffle.partitions to 2,048 partitions (1TB*1024*1024/512), ingest the data, execute the narrow transformations, optimize the data by sorting it (which automatically repartitions the data), and then write to parquet.', 'C.Set spark.sql.adaptive.advisoryPartitionSizeInBytes to 512 MB bytes, ingest the data, execute the narrow transformations, coalesce to 2,048 partitions (1TB*1024*1024/512), and then write to parquet.', 'D.Ingest the data, execute the narrow transformations, repartition to 2,048 partitions (1TB* 1024*1024/512), and then write to parquet.', 'E.Set spark.sql.shuffle.partitions to 512, ingest the data, execute the narrow transformations, and then write to parquet.']","['A.Set spark.sql.files.maxPartitionBytes to 512 MB, ingest the data, execute the narrow transformations, and then write to parquet.']",[],
71,examtopics databricks data engineer professional certification question 71,https://www.examtopics.com/discussions/databricks/view/124595-exam-certified-data-engineer-professional-topic-1-question/,"A junior data engineer has been asked to develop a streaming data pipeline with a grouped aggregation using DataFrame df. The pipeline needs to calculate the average humidity and average temperature for each non-overlapping five-minute interval. Incremental state information should be maintained for 10 minutes for late-arriving data.Streaming DataFrame df has the following schema:""device_id INT, event_time TIMESTAMP, temp FLOAT, humidity FLOAT""Code block:Choose the response that correctly fills in the blank within the code block to complete this task.","['A.withWatermark(""event_time"", ""10 minutes"")', 'B.awaitArrival(""event_time"", ""10 minutes"")', 'C.await(""event_time + ‘10 minutes\'"")', 'D.slidingWindow(""event_time"", ""10 minutes"")', 'E.delayWrite(""event_time"", ""10 minutes"")']","['A.withWatermark(""event_time"", ""10 minutes"")']",['https://img.examtopics.com/certified-data-engineer-professional/image34.png'],
72,examtopics databricks data engineer professional certification question 72,https://www.examtopics.com/discussions/databricks/view/129174-exam-certified-data-engineer-professional-topic-1-question/,"A data team's Structured Streaming job is configured to calculate running aggregates for item sales to update a downstream marketing dashboard. The marketing team has introduced a new promotion, and they would like to add a new field to track the number of times this promotion code is used for each item. A junior data engineer suggests updating the existing query as follows. Note that proposed changes are in bold.Original query:Proposed query:Proposed query:.start(“/item_agg”)Which step must also be completed to put the proposed query into production?","['A.Specify a new checkpointLocation', 'B.Increase the shuffle partitions to account for additional aggregates', ""C.Run REFRESH TABLE delta.'/item_agg'"", 'D.Register the data in the ""/item_agg"" directory to the Hive metastore', 'E.Remove .option(‘mergeSchema’, ‘true’) from the streaming write']",['A.Specify a new checkpointLocation'],"['https://img.examtopics.com/certified-data-engineer-professional/image35.png', 'https://img.examtopics.com/certified-data-engineer-professional/image36.png']",
73,examtopics databricks data engineer professional certification question 73,https://www.examtopics.com/discussions/databricks/view/126756-exam-certified-data-engineer-professional-topic-1-question/,"A Structured Streaming job deployed to production has been resulting in higher than expected cloud storage costs. At present, during normal execution, each microbatch of data is processed in less than 3s; at least 12 times per minute, a microbatch is processed that contains 0 records. The streaming write was configured using the default trigger settings. The production job is currently scheduled alongside many other Databricks jobs in a workspace with instance pools provisioned to reduce start-up time for jobs with batch execution.Holding all other variables constant and assuming records need to be processed in less than 10 minutes, which adjustment will meet the requirement?","['A.Set the trigger interval to 3 seconds; the default trigger interval is consuming too many records per batch, resulting in spill to disk that can increase volume costs.', 'B.Increase the number of shuffle partitions to maximize parallelism, since the trigger interval cannot be modified without modifying the checkpoint directory.', 'C.Set the trigger interval to 10 minutes; each batch calls APIs in the source storage account, so decreasing trigger frequency to maximum allowable threshold should minimize this cost.', 'D.Set the trigger interval to 500 milliseconds; setting a small but non-zero trigger interval ensures that the source is not queried too frequently.', 'E.Use the trigger once option and configure a Databricks job to execute the query every 10 minutes; this approach minimizes costs for both compute and storage.']","['C.Set the trigger interval to 10 minutes; each batch calls APIs in the source storage account, so decreasing trigger frequency to maximum allowable threshold should minimize this cost.']",[],
74,examtopics databricks data engineer professional certification question 74,https://www.examtopics.com/discussions/databricks/view/124596-exam-certified-data-engineer-professional-topic-1-question/,Which statement describes the correct use of pyspark.sql.functions.broadcast?,"['A.It marks a column as having low enough cardinality to properly map distinct values to available partitions, allowing a broadcast join.', 'B.It marks a column as small enough to store in memory on all executors, allowing a broadcast join.', 'C.It caches a copy of the indicated table on attached storage volumes for all active clusters within a Databricks workspace.', 'D.It marks a DataFrame as small enough to store in memory on all executors, allowing a broadcast join.', 'E.It caches a copy of the indicated table on all nodes in the cluster for use in all future queries during the cluster lifetime.']","['D.It marks a DataFrame as small enough to store in memory on all executors, allowing a broadcast join.']",[],
75,examtopics databricks data engineer professional certification question 75,https://www.examtopics.com/discussions/databricks/view/124369-exam-certified-data-engineer-professional-topic-1-question/,"A data engineer is configuring a pipeline that will potentially see late-arriving, duplicate records.In addition to de-duplicating records within the batch, which of the following approaches allows the data engineer to deduplicate data against previously processed records as it is inserted into a Delta table?","['A.Set the configuration delta.deduplicate = true.', 'B.VACUUM the Delta table after each batch completes.', 'C.Perform an insert-only merge with a matching condition on a unique key.', 'D.Perform a full outer join on a unique key and overwrite existing data.', 'E.Rely on Delta Lake schema enforcement to prevent duplicate records.']",['C.Perform an insert-only merge with a matching condition on a unique key.'],[],
76,examtopics databricks data engineer professional certification question 76,https://www.examtopics.com/discussions/databricks/view/129709-exam-certified-data-engineer-professional-topic-1-question/,"A data pipeline uses Structured Streaming to ingest data from Apache Kafka to Delta Lake. Data is being stored in a bronze table, and includes the Kafka-generated timestamp, key, and value. Three months after the pipeline is deployed, the data engineering team has noticed some latency issues during certain times of the day.A senior data engineer updates the Delta Table's schema and ingestion logic to include the current timestamp (as recorded by Apache Spark) as well as the Kafka topic and partition. The team plans to use these additional metadata fields to diagnose the transient processing delays.Which limitation will the team face while diagnosing this problem?","['A.New fields will not be computed for historic records.', 'B.Spark cannot capture the topic and partition fields from a Kafka source.', 'C.New fields cannot be added to a production Delta table.', 'D.Updating the table schema will invalidate the Delta transaction log metadata.', 'E.Updating the table schema requires a default value provided for each field added.']",['A.New fields will not be computed for historic records.'],[],
77,examtopics databricks data engineer professional certification question 77,https://www.examtopics.com/discussions/databricks/view/124597-exam-certified-data-engineer-professional-topic-1-question/,"In order to facilitate near real-time workloads, a data engineer is creating a helper function to leverage the schema detection and evolution functionality of Databricks Auto Loader. The desired function will automatically detect the schema of the source directly, incrementally process JSON files as they arrive in a source directory, and automatically evolve the schema of the table when new fields are detected.The function is displayed below with a blank:Which response correctly fills in the blank to meet the specified requirements?","['A.', 'B.', 'C.', 'D.', 'E.']",['E.'],['https://img.examtopics.com/certified-data-engineer-professional/image37.png'],
78,examtopics databricks data engineer professional certification question 78,https://www.examtopics.com/discussions/databricks/view/124598-exam-certified-data-engineer-professional-topic-1-question/,"The data engineering team maintains the following code:Assuming that this code produces logically correct results and the data in the source table has been de-duplicated and validated, which statement describes what will occur when this code is executed?","['A.The silver_customer_sales table will be overwritten by aggregated values calculated from all records in the gold_customer_lifetime_sales_summary table as a batch job.', 'B.A batch job will update the gold_customer_lifetime_sales_summary table, replacing only those rows that have different values than the current version of the table, using customer_id as the primary key.', 'C.The gold_customer_lifetime_sales_summary table will be overwritten by aggregated values calculated from all records in the silver_customer_sales table as a batch job.', 'D.An incremental job will leverage running information in the state store to update aggregate values in the gold_customer_lifetime_sales_summary table.', 'E.An incremental job will detect if new rows have been written to the silver_customer_sales table; if new rows are detected, all aggregates will be recalculated and used to overwrite the gold_customer_lifetime_sales_summary table.']",['C.The gold_customer_lifetime_sales_summary table will be overwritten by aggregated values calculated from all records in the silver_customer_sales table as a batch job.'],['https://img.examtopics.com/certified-data-engineer-professional/image43.png'],
79,examtopics databricks data engineer professional certification question 79,https://www.examtopics.com/discussions/databricks/view/124599-exam-certified-data-engineer-professional-topic-1-question/,"The data architect has mandated that all tables in the Lakehouse should be configured as external (also known as ""unmanaged"") Delta Lake tables.Which approach will ensure that this requirement is met?","['A.When a database is being created, make sure that the LOCATION keyword is used.', 'B.When configuring an external data warehouse for all table storage, leverage Databricks for all ELT.', 'C.When data is saved to a table, make sure that a full file path is specified alongside the Delta format.', 'D.When tables are created, make sure that the EXTERNAL keyword is used in the CREATE TABLE statement.', 'E.When the workspace is being configured, make sure that external cloud object storage has been mounted.']","['C.When data is saved to a table, make sure that a full file path is specified alongside the Delta format.']",[],
80,examtopics databricks data engineer professional certification question 80,https://www.examtopics.com/discussions/databricks/view/129728-exam-certified-data-engineer-professional-topic-1-question/,"The marketing team is looking to share data in an aggregate table with the sales organization, but the field names used by the teams do not match, and a number of marketing-specific fields have not been approved for the sales org.Which of the following solutions addresses the situation while emphasizing simplicity?","['A.Create a view on the marketing table selecting only those fields approved for the sales team; alias the names of any fields that should be standardized to the sales naming conventions.', ""B.Create a new table with the required schema and use Delta Lake's DEEP CLONE functionality to sync up changes committed to one table to the corresponding table."", 'C.Use a CTAS statement to create a derivative table from the marketing table; configure a production job to propagate changes.', 'D.Add a parallel table write to the current production pipeline, updating a new sales table that varies as required from the marketing table.', 'E.Instruct the marketing team to download results as a CSV and email them to the sales organization.']",['A.Create a view on the marketing table selecting only those fields approved for the sales team; alias the names of any fields that should be standardized to the sales naming conventions.'],[],
81,examtopics databricks data engineer professional certification question 81,https://www.examtopics.com/discussions/databricks/view/124255-exam-certified-data-engineer-professional-topic-1-question/,"A CHECK constraint has been successfully added to the Delta table named activity_details using the following logic:A batch job is attempting to insert new records to the table, including a record where latitude = 45.50 and longitude = 212.67.Which statement describes the outcome of this batch insert?","['A.The write will fail when the violating record is reached; any records previously processed will be recorded to the target table.', 'B.The write will fail completely because of the constraint violation and no records will be inserted into the target table.', 'C.The write will insert all records except those that violate the table constraints; the violating records will be recorded to a quarantine table.', 'D.The write will include all records in the target table; any violations will be indicated in the boolean column named valid_coordinates.', 'E.The write will insert all records except those that violate the table constraints; the violating records will be reported in a warning log.']",['B.The write will fail completely because of the constraint violation and no records will be inserted into the target table.'],['https://img.examtopics.com/certified-data-engineer-professional/image44.png'],
82,examtopics databricks data engineer professional certification question 82,https://www.examtopics.com/discussions/databricks/view/124600-exam-certified-data-engineer-professional-topic-1-question/,"A junior data engineer has manually configured a series of jobs using the Databricks Jobs UI. Upon reviewing their work, the engineer realizes that they are listed as the ""Owner"" for each job. They attempt to transfer ""Owner"" privileges to the ""DevOps"" group, but cannot successfully accomplish this task.Which statement explains what is preventing this privilege transfer?","['A.Databricks jobs must have exactly one owner; ""Owner"" privileges cannot be assigned to a group.', 'B.The creator of a Databricks job will always have ""Owner"" privileges; this configuration cannot be changed.', 'C.Other than the default ""admins"" group, only individual users can be granted privileges on jobs.', 'D.A user can only transfer job ownership to a group if they are also a member of that group.', 'E.Only workspace administrators can grant ""Owner"" privileges to a group.']","['A.Databricks jobs must have exactly one owner; ""Owner"" privileges cannot be assigned to a group.']",[],
83,examtopics databricks data engineer professional certification question 83,https://www.examtopics.com/discussions/databricks/view/124601-exam-certified-data-engineer-professional-topic-1-question/,"All records from an Apache Kafka producer are being ingested into a single Delta Lake table with the following schema:key BINARY, value BINARY, topic STRING, partition LONG, offset LONG, timestamp LONGThere are 5 unique topics being ingested. Only the ""registration"" topic contains Personal Identifiable Information (PII). The company wishes to restrict access to PII. The company also wishes to only retain records containing PII in this table for 14 days after initial ingestion. However, for non-PII information, it would like to retain these records indefinitely.Which of the following solutions meets the requirements?","[""A.All data should be deleted biweekly; Delta Lake's time travel functionality should be leveraged to maintain a history of non-PII information."", 'B.Data should be partitioned by the registration field, allowing ACLs and delete statements to be set for the PII directory.', 'C.Because the value field is stored as binary data, this information is not considered PII and no special precautions should be taken.', 'D.Separate object storage containers should be specified based on the partition field, allowing isolation at the storage level.', 'E.Data should be partitioned by the topic field, allowing ACLs and delete statements to leverage partition boundaries.']","['E.Data should be partitioned by the topic field, allowing ACLs and delete statements to leverage partition boundaries.']",[],
84,examtopics databricks data engineer professional certification question 84,https://www.examtopics.com/discussions/databricks/view/124602-exam-certified-data-engineer-professional-topic-1-question/,"The data architect has decided that once data has been ingested from external sources into theDatabricks Lakehouse, table access controls will be leveraged to manage permissions for all production tables and views.The following logic was executed to grant privileges for interactive queries on a production database to the core engineering group.GRANT USAGE ON DATABASE prod TO eng;GRANT SELECT ON DATABASE prod TO eng;Assuming these are the only privileges that have been granted to the eng group and that these users are not workspace administrators, which statement describes their privileges?","['A.Group members have full permissions on the prod database and can also assign permissions to other users or groups.', 'B.Group members are able to list all tables in the prod database but are not able to see the results of any queries on those tables.', 'C.Group members are able to query and modify all tables and views in the prod database, but cannot create new tables or views.', 'D.Group members are able to query all tables and views in the prod database, but cannot create or edit anything in the database.', 'E.Group members are able to create, query, and modify all tables and views in the prod database, but cannot define custom functions.']","['D.Group members are able to query all tables and views in the prod database, but cannot create or edit anything in the database.']",[],
85,examtopics databricks data engineer professional certification question 85,https://www.examtopics.com/discussions/databricks/view/124604-exam-certified-data-engineer-professional-topic-1-question/,"A distributed team of data analysts share computing resources on an interactive cluster with autoscaling configured. In order to better manage costs and query throughput, the workspace administrator is hoping to evaluate whether cluster upscaling is caused by many concurrent users or resource-intensive queries.In which location can one review the timeline for cluster resizing events?","['A.Workspace audit logs', ""B.Driver's log file"", 'C.Ganglia', 'D.Cluster Event Log', ""E.Executor's log file""]",['D.Cluster Event Log'],[],
86,examtopics databricks data engineer professional certification question 86,https://www.examtopics.com/discussions/databricks/view/125408-exam-certified-data-engineer-professional-topic-1-question/,"When evaluating the Ganglia Metrics for a given cluster with 3 executor nodes, which indicator would signal proper utilization of the VM's resources?","['A.The five Minute Load Average remains consistent/flat', 'B.Bytes Received never exceeds 80 million bytes per second', 'C.Network I/O never spikes', 'D.Total Disk Space remains constant', 'E.CPU Utilization is around 75%']",['E.CPU Utilization is around 75%'],[],
87,examtopics databricks data engineer professional certification question 87,https://www.examtopics.com/discussions/databricks/view/124605-exam-certified-data-engineer-professional-topic-1-question/,Which of the following technologies can be used to identify key areas of text when parsing Spark Driver log4j output?,"['A.Regex', 'B.Julia', 'C.pyspsark.ml.feature', 'D.Scala Datasets', 'E.C++']",['A.Regex'],[],
88,examtopics databricks data engineer professional certification question 88,https://www.examtopics.com/discussions/databricks/view/130110-exam-certified-data-engineer-professional-topic-1-question/,"You are testing a collection of mathematical functions, one of which calculates the area under a curve as described by another function.assert(myIntegrate(lambda x: x*x, 0, 3) [0] == 9)Which kind of test would the above line exemplify?","['A.Unit', 'B.Manual', 'C.Functional', 'D.Integration', 'E.End-to-end']",['A.Unit'],[],
89,examtopics databricks data engineer professional certification question 89,https://www.examtopics.com/discussions/databricks/view/124606-exam-certified-data-engineer-professional-topic-1-question/,"A Databricks job has been configured with 3 tasks, each of which is a Databricks notebook. Task A does not depend on other tasks. Tasks B and C run in parallel, with each having a serial dependency on Task A.If task A fails during a scheduled run, which statement describes the results of this run?","['A.Because all tasks are managed as a dependency graph, no changes will be committed to the Lakehouse until all tasks have successfully been completed.', 'B.Tasks B and C will attempt to run as configured; any changes made in task A will be rolled back due to task failure.', 'C.Unless all tasks complete successfully, no changes will be committed to the Lakehouse; because task A failed, all commits will be rolled back automatically.', 'D.Tasks B and C will be skipped; some logic expressed in task A may have been committed before task failure.', 'E.Tasks B and C will be skipped; task A will not commit any changes because of stage failure.']",['D.Tasks B and C will be skipped; some logic expressed in task A may have been committed before task failure.'],[],
90,examtopics databricks data engineer professional certification question 90,https://www.examtopics.com/discussions/databricks/view/127597-exam-certified-data-engineer-professional-topic-1-question/,Which statement regarding Spark configuration on the Databricks platform is true?,"['A.The Databricks REST API can be used to modify the Spark configuration properties for an interactive cluster without interrupting jobs currently running on the cluster.', 'B.Spark configurations set within a notebook will affect all SparkSessions attached to the same interactive cluster.', 'C.Spark configuration properties can only be set for an interactive cluster by creating a global init script.', 'D.Spark configuration properties set for an interactive cluster with the Clusters UI will impact all notebooks attached to that cluster.', 'E.When the same Spark configuration property is set for an interactive cluster and a notebook attached to that cluster, the notebook setting will always be ignored.']",['D.Spark configuration properties set for an interactive cluster with the Clusters UI will impact all notebooks attached to that cluster.'],[],
91,examtopics databricks data engineer professional certification question 91,https://www.examtopics.com/discussions/databricks/view/130091-exam-certified-data-engineer-professional-topic-1-question/,"A developer has successfully configured their credentials for Databricks Repos and cloned a remote Git repository. They do not have privileges to make changes to the main branch, which is the only branch currently visible in their workspace.Which approach allows this user to share their code updates without the risk of overwriting the work of their teammates?","['A.Use Repos to checkout all changes and send the git diff log to the team.', 'B.Use Repos to create a fork of the remote repository, commit all changes, and make a pull request on the source repository.', 'C.Use Repos to pull changes from the remote Git repository; commit and push changes to a branch that appeared as changes were pulled.', 'D.Use Repos to merge all differences and make a pull request back to the remote repository.', 'E.Use Repos to create a new branch, commit all changes, and push changes to the remote Git repository.']","['E.Use Repos to create a new branch, commit all changes, and push changes to the remote Git repository.']",[],
92,examtopics databricks data engineer professional certification question 92,https://www.examtopics.com/discussions/databricks/view/126210-exam-certified-data-engineer-professional-topic-1-question/,"In order to prevent accidental commits to production data, a senior data engineer has instituted a policy that all development work will reference clones of Delta Lake tables. After testing both DEEP and SHALLOW CLONE, development tables are created using SHALLOW CLONE.A few weeks after initial table creation, the cloned versions of several tables implemented as Type 1 Slowly Changing Dimension (SCD) stop working. The transaction logs for the source tables show that VACUUM was run the day before.Which statement describes why the cloned tables are no longer working?","['A.Because Type 1 changes overwrite existing records, Delta Lake cannot guarantee data consistency for cloned tables.', 'B.Running VACUUM automatically invalidates any shallow clones of a table; DEEP CLONE should always be used when a cloned table will be repeatedly queried.', 'C.Tables created with SHALLOW CLONE are automatically deleted after their default retention threshold of 7 days.', 'D.The metadata created by the CLONE operation is referencing data files that were purged as invalid by the VACUUM command.', 'E.The data files compacted by VACUUM are not tracked by the cloned metadata; running REFRESH on the cloned table will pull in recent changes.']",['D.The metadata created by the CLONE operation is referencing data files that were purged as invalid by the VACUUM command.'],[],
93,examtopics databricks data engineer professional certification question 93,https://www.examtopics.com/discussions/databricks/view/127268-exam-certified-data-engineer-professional-topic-1-question/,You are performing a join operation to combine values from a static userLookup table with a streaming DataFrame streamingDF.Which code block attempts to perform an invalid stream-static join?,"['A.userLookup.join(streamingDF, [""userid""], how=""inner"")', 'B.streamingDF.join(userLookup, [""user_id""], how=""outer"")', 'C.streamingDF.join(userLookup, [""user_id”], how=""left"")', 'D.streamingDF.join(userLookup, [""userid""], how=""inner"")', 'E.userLookup.join(streamingDF, [""user_id""], how=""right"")']","['B.streamingDF.join(userLookup, [""user_id""], how=""outer"")']",[],
94,examtopics databricks data engineer professional certification question 94,https://www.examtopics.com/discussions/databricks/view/126236-exam-certified-data-engineer-professional-topic-1-question/,"Spill occurs as a result of executing various wide transformations. However, diagnosing spill requires one to proactively look for key indicators.Where in the Spark UI are two of the primary indicators that a partition is spilling to disk?","['A.Query’s detail screen and Job’s detail screen', 'B.Stage’s detail screen and Executor’s log files', 'C.Driver’s and Executor’s log files', 'D.Executor’s detail screen and Executor’s log files', 'E.Stage’s detail screen and Query’s detail screen']",['B.Stage’s detail screen and Executor’s log files'],[],
95,examtopics databricks data engineer professional certification question 95,https://www.examtopics.com/discussions/databricks/view/128995-exam-certified-data-engineer-professional-topic-1-question/,"A task orchestrator has been configured to run two hourly tasks. First, an outside system writes Parquet data to a directory mounted at /mnt/raw_orders/. After this data is written, a Databricks job containing the following code is executed:Assume that the fields customer_id and order_id serve as a composite key to uniquely identify each order, and that the time field indicates when the record was queued in the source system.If the upstream system is known to occasionally enqueue duplicate entries for a single order hours apart, which statement is correct?","['A.Duplicate records enqueued more than 2 hours apart may be retained and the orders table may contain duplicate records with the same customer_id and order_id.', 'B.All records will be held in the state store for 2 hours before being deduplicated and committed to the orders table.', 'C.The orders table will contain only the most recent 2 hours of records and no duplicates will be present.', 'D.Duplicate records arriving more than 2 hours apart will be dropped, but duplicates that arrive in the same batch may both be written to the orders table.', 'E.The orders table will not contain duplicates, but records arriving more than 2 hours late will be ignored and missing from the table.']",['A.Duplicate records enqueued more than 2 hours apart may be retained and the orders table may contain duplicate records with the same customer_id and order_id.'],['https://img.examtopics.com/certified-data-engineer-professional/image45.png'],
96,examtopics databricks data engineer professional certification question 96,https://www.examtopics.com/discussions/databricks/view/126259-exam-certified-data-engineer-professional-topic-1-question/,"A junior data engineer is migrating a workload from a relational database system to the Databricks Lakehouse. The source system uses a star schema, leveraging foreign key constraints and multi-table inserts to validate records on write.Which consideration will impact the decisions made by the engineer while migrating this workload?","['A.Databricks only allows foreign key constraints on hashed identifiers, which avoid collisions in highly-parallel writes.', 'B.Databricks supports Spark SQL and JDBC; all logic can be directly migrated from the source system without refactoring.', 'C.Committing to multiple tables simultaneously requires taking out multiple table locks and can lead to a state of deadlock.', 'D.All Delta Lake transactions are ACID compliant against a single table, and Databricks does not enforce foreign key constraints.', 'E.Foreign keys must reference a primary key field; multi-table inserts must leverage Delta Lake’s upsert functionality.']","['D.All Delta Lake transactions are ACID compliant against a single table, and Databricks does not enforce foreign key constraints.']",[],
97,examtopics databricks data engineer professional certification question 97,https://www.examtopics.com/discussions/databricks/view/126260-exam-certified-data-engineer-professional-topic-1-question/,"A data architect has heard about Delta Lake’s built-in versioning and time travel capabilities. For auditing purposes, they have a requirement to maintain a full record of all valid street addresses as they appear in the customers table.The architect is interested in implementing a Type 1 table, overwriting existing records with new values and relying on Delta Lake time travel to support long-term auditing. A data engineer on the project feels that a Type 2 table will provide better performance and scalability.Which piece of information is critical to this decision?","['A.Data corruption can occur if a query fails in a partially completed state because Type 2 tables require setting multiple fields in a single update.', 'B.Shallow clones can be combined with Type 1 tables to accelerate historic queries for long-term versioning.', 'C.Delta Lake time travel cannot be used to query previous versions of these tables because Type 1 changes modify data files in place.', 'D.Delta Lake time travel does not scale well in cost or latency to provide a long-term versioning solution.', 'E.Delta Lake only supports Type 0 tables; once records are inserted to a Delta Lake table, they cannot be modified.']",['D.Delta Lake time travel does not scale well in cost or latency to provide a long-term versioning solution.'],[],
98,examtopics databricks data engineer professional certification question 98,https://www.examtopics.com/discussions/databricks/view/128986-exam-certified-data-engineer-professional-topic-1-question/,"A table named user_ltv is being used to create a view that will be used by data analysts on various teams. Users in the workspace are configured into groups, which are used for setting up data access using ACLs.The user_ltv table has the following schema:email STRING, age INT, ltv INTThe following view definition is executed:An analyst who is not a member of the auditing group executes the following query:SELECT * FROM user_ltv_no_minorsWhich statement describes the results returned by this query?","['A.All columns will be displayed normally for those records that have an age greater than 17; records not meeting this condition will be omitted.', 'B.All age values less than 18 will be returned as null values, all other columns will be returned with the values in user_ltv.', 'C.All values for the age column will be returned as null values, all other columns will be returned with the values in user_ltv.', 'D.All records from all columns will be displayed with the values in user_ltv.', 'E.All columns will be displayed normally for those records that have an age greater than 18; records not meeting this condition will be omitted.']",['A.All columns will be displayed normally for those records that have an age greater than 17; records not meeting this condition will be omitted.'],['https://img.examtopics.com/certified-data-engineer-professional/image46.png'],
99,examtopics databricks data engineer professional certification question 99,https://www.examtopics.com/discussions/databricks/view/126268-exam-certified-data-engineer-professional-topic-1-question/,"The data governance team is reviewing code used for deleting records for compliance with GDPR. The following logic has been implemented to propagate delete requests from the user_lookup table to the user_aggregates table.Assuming that user_id is a unique identifying key and that all users that have requested deletion have been removed from the user_lookup table, which statement describes whether successfully executing the above logic guarantees that the records to be deleted from the user_aggregates table are no longer accessible and why?","['A.No; the Delta Lake DELETE command only provides ACID guarantees when combined with the MERGE INTO command.', 'B.No; files containing deleted records may still be accessible with time travel until a VACUUM command is used to remove invalidated data files.', 'C.Yes; the change data feed uses foreign keys to ensure delete consistency throughout the Lakehouse.', 'D.Yes; Delta Lake ACID guarantees provide assurance that the DELETE command succeeded fully and permanently purged these records.', 'E.No; the change data feed only tracks inserts and updates, not deleted records.']",['B.No; files containing deleted records may still be accessible with time travel until a VACUUM command is used to remove invalidated data files.'],['https://img.examtopics.com/certified-data-engineer-professional/image47.png'],
100,examtopics databricks data engineer professional certification question 100,https://www.examtopics.com/discussions/databricks/view/127269-exam-certified-data-engineer-professional-topic-1-question/,"The data engineering team has been tasked with configuring connections to an external database that does not have a supported native connector with Databricks. The external database already has data security configured by group membership. These groups map directly to user groups already created in Databricks that represent various teams within the company.A new login credential has been created for each group in the external database. The Databricks Utilities Secrets module will be used to make these credentials available to Databricks users.Assuming that all the credentials are configured correctly on the external database and group membership is properly configured on Databricks, which statement describes how teams can be granted the minimum necessary access to using these credentials?","['A.""Manage"" permissions should be set on a secret key mapped to those credentials that will be used by a given team.', 'B.""Read"" permissions should be set on a secret key mapped to those credentials that will be used by a given team.', 'C.""Read"" permissions should be set on a secret scope containing only those credentials that will be used by a given team.', 'D.""Manage"" permissions should be set on a secret scope containing only those credentials that will be used by a given team.No additional configuration is necessary as long as all users are configured as administrators in the workspace where secrets have been added.']","['C.""Read"" permissions should be set on a secret scope containing only those credentials that will be used by a given team.']",[],
101,examtopics databricks data engineer professional certification question 101,https://www.examtopics.com/discussions/databricks/view/132982-exam-certified-data-engineer-professional-topic-1-question/,Which indicators would you look for in the Spark UI’s Storage tab to signal that a cached table is not performing optimally? Assume you are using Spark’s MEMORY_ONLY storage level.,"['A.Size on Disk is < Size in Memory', 'B.The RDD Block Name includes the “*” annotation signaling a failure to cache', 'C.Size on Disk is > 0', 'D.The number of Cached Partitions > the number of Spark Partitions', 'E.On Heap Memory Usage is within 75% of Off Heap Memory Usage']",['C.Size on Disk is > 0'],[],
102,examtopics databricks data engineer professional certification question 102,https://www.examtopics.com/discussions/databricks/view/126297-exam-certified-data-engineer-professional-topic-1-question/,What is the first line of a Databricks Python notebook when viewed in a text editor?,"['A.%python', 'B.// Databricks notebook source', 'C.# Databricks notebook source', 'D.-- Databricks notebook source', 'E.# MAGIC %python']",['C.# Databricks notebook source'],[],
103,examtopics databricks data engineer professional certification question 103,https://www.examtopics.com/discussions/databricks/view/131948-exam-certified-data-engineer-professional-topic-1-question/,Which statement describes a key benefit of an end-to-end test?,"['A.Makes it easier to automate your test suite', 'B.Pinpoints errors in the building blocks of your application', 'C.Provides testing coverage for all code paths and branches', 'D.Closely simulates real world usage of your application', 'E.Ensures code is optimized for a real-life workflow']",['D.Closely simulates real world usage of your application'],[],
104,examtopics databricks data engineer professional certification question 104,https://www.examtopics.com/discussions/databricks/view/131955-exam-certified-data-engineer-professional-topic-1-question/,The Databricks CLI is used to trigger a run of an existing job by passing the job_id parameter. The response that the job run request has been submitted successfully includes a field run_id.Which statement describes what the number alongside this field represents?,"['A.The job_id and number of times the job has been run are concatenated and returned.', 'B.The total number of jobs that have been run in the workspace.', 'C.The number of times the job definition has been run in this workspace.', 'D.The job_id is returned in this field.', 'E.The globally unique ID of the newly triggered run.']",['E.The globally unique ID of the newly triggered run.'],[],
105,examtopics databricks data engineer professional certification question 105,https://www.examtopics.com/discussions/databricks/view/126298-exam-certified-data-engineer-professional-topic-1-question/,"The data science team has created and logged a production model using MLflow. The model accepts a list of column names and returns a new column of type DOUBLE.The following code correctly imports the production model, loads the customers table containing the customer_id key column into a DataFrame, and defines the feature columns needed for the model.Which code block will output a DataFrame with the schema ""customer_id LONG, predictions DOUBLE""?","['A.df.map(lambda x:model(x[columns])).select(""customer_id, predictions"")', 'B.df.select(""customer_id"", model(*columns).alias(""predictions""))', 'C.model.predict(df, columns)', 'D.df.select(""customer_id"", pandas_udf(model, columns).alias(""predictions""))', 'E.df.apply(model, columns).select(""customer_id, predictions"")']","['B.df.select(""customer_id"", model(*columns).alias(""predictions""))']",['https://img.examtopics.com/certified-data-engineer-professional/image48.png'],
106,examtopics databricks data engineer professional certification question 106,https://www.examtopics.com/discussions/databricks/view/130120-exam-certified-data-engineer-professional-topic-1-question/,"A nightly batch job is configured to ingest all data files from a cloud object storage container where records are stored in a nested directory structure YYYY/MM/DD. The data for each date represents all records that were processed by the source system on that date, noting that some records may be delayed as they await moderator approval. Each entry represents a user review of a product and has the following schema:user_id STRING, review_id BIGINT, product_id BIGINT, review_timestamp TIMESTAMP, review_text STRINGThe ingestion job is configured to append all data for the previous date to a target table reviews_raw with an identical schema to the source system. The next step in the pipeline is a batch write to propagate all new records inserted into reviews_raw to a table where data is fully deduplicated, validated, and enriched.Which solution minimizes the compute costs to propagate this batch of data?","['A.Perform a batch read on the reviews_raw table and perform an insert-only merge using the natural composite key user_id, review_id, product_id, review_timestamp.', 'B.Configure a Structured Streaming read against the reviews_raw table using the trigger once execution mode to process new records as a batch job.', 'C.Use Delta Lake version history to get the difference between the latest version of reviews_raw and one version prior, then write these records to the next table.', 'D.Filter all records in the reviews_raw table based on the review_timestamp; batch append those records produced in the last 48 hours.', 'E.Reprocess all records in reviews_raw and overwrite the next table in the pipeline.']","['A.Perform a batch read on the reviews_raw table and perform an insert-only merge using the natural composite key user_id, review_id, product_id, review_timestamp.']",[],
107,examtopics databricks data engineer professional certification question 107,https://www.examtopics.com/discussions/databricks/view/128996-exam-certified-data-engineer-professional-topic-1-question/,Which statement describes Delta Lake optimized writes?,"['A.Before a Jobs cluster terminates, OPTIMIZE is executed on all tables modified during the most recent job.', 'B.An asynchronous job runs after the write completes to detect if files could be further compacted; if yes, an OPTIMIZE job is executed toward a default of 1 GB.', 'C.Data is queued in a messaging bus instead of committing data directly to memory; all data is committed from the messaging bus in one batch once the job is complete.', 'D.Optimized writes use logical partitions instead of directory partitions; because partition boundaries are only represented in metadata, fewer small files are written.', 'E.A shuffle occurs prior to writing to try to group similar data together resulting in fewer files instead of each executor writing multiple files based on directory partitions.']",['E.A shuffle occurs prior to writing to try to group similar data together resulting in fewer files instead of each executor writing multiple files based on directory partitions.'],[],
108,examtopics databricks data engineer professional certification question 108,https://www.examtopics.com/discussions/databricks/view/132138-exam-certified-data-engineer-professional-topic-1-question/,Which statement describes the default execution mode for Databricks Auto Loader?,"['A.Cloud vendor-specific queue storage and notification services are configured to track newly arriving files; the target table is materialized by directly querying all valid files in the source directory.', 'B.New files are identified by listing the input directory; the target table is materialized by directly querying all valid files in the source directory.', 'C.Webhooks trigger a Databricks job to run anytime new data arrives in a source directory; new data are automatically merged into target tables using rules inferred from the data.', 'D.New files are identified by listing the input directory; new files are incrementally and idempotently loaded into the target Delta Lake table.', 'E.Cloud vendor-specific queue storage and notification services are configured to track newly arriving files; new files are incrementally and idempotently loaded into the target Delta Lake table.']",['D.New files are identified by listing the input directory; new files are incrementally and idempotently loaded into the target Delta Lake table.'],[],
109,examtopics databricks data engineer professional certification question 109,https://www.examtopics.com/discussions/databricks/view/132980-exam-certified-data-engineer-professional-topic-1-question/,"A Delta Lake table representing metadata about content posts from users has the following schema:user_id LONG, post_text STRING, post_id STRING, longitude FLOAT, latitude FLOAT, post_time TIMESTAMP, date DATEBased on the above schema, which column is a good candidate for partitioning the Delta Table?","['A.post_time', 'B.latitude', 'C.post_id', 'D.user_id', 'E.date']",['E.date'],[],
110,examtopics databricks data engineer professional certification question 110,https://www.examtopics.com/discussions/databricks/view/126796-exam-certified-data-engineer-professional-topic-1-question/,A large company seeks to implement a near real-time solution involving hundreds of pipelines with parallel updates of many tables with extremely high volume and high velocity data.Which of the following solutions would you implement to achieve this requirement?,"['A.Use Databricks High Concurrency clusters, which leverage optimized cloud storage connections to maximize data throughput.', 'B.Partition ingestion tables by a small time duration to allow for many data files to be written in parallel.', 'C.Configure Databricks to save all data to attached SSD volumes instead of object storage, increasing file I/O significantly.', 'D.Isolate Delta Lake tables in their own storage containers to avoid API limits imposed by cloud vendors.', 'E.Store all tables in a single database to ensure that the Databricks Catalyst Metastore can load balance overall throughput.']",['B.Partition ingestion tables by a small time duration to allow for many data files to be written in parallel.'],[],
111,examtopics databricks data engineer professional certification question 111,https://www.examtopics.com/discussions/databricks/view/126313-exam-certified-data-engineer-professional-topic-1-question/,Which describes a method of installing a Python package scoped at the notebook level to all nodes in the currently active cluster?,"['A.Run source env/bin/activate in a notebook setup script', 'B.Use b in a notebook cell', 'C.Use %pip install in a notebook cell', 'D.Use %sh pip install in a notebook cell', 'E.Install libraries from PyPI using the cluster UI']",['C.Use %pip install in a notebook cell'],[],
112,examtopics databricks data engineer professional certification question 112,https://www.examtopics.com/discussions/databricks/view/135995-exam-certified-data-engineer-professional-topic-1-question/,"Each configuration below is identical to the extent that each cluster has 400 GB total of RAM 160 total cores and only one Executor per VM.Given an extremely long-running job for which completion must be guaranteed, which cluster configuration will be able to guarantee completion of the job in light of one or more VM failures?","['A.• Total VMs: 8• 50 GB per Executor• 20 Cores / Executor', 'B.• Total VMs: 16• 25 GB per Executor• 10 Cores / Executor', 'C.• Total VMs: 1• 400 GB per Executor• 160 Cores/Executor', 'D.• Total VMs: 4• 100 GB per Executor• 40 Cores / Executor', 'E.• Total VMs: 2• 200 GB per Executor• 80 Cores / Executor']",['B.• Total VMs: 16• 25 GB per Executor• 10 Cores / Executor'],[],
113,examtopics databricks data engineer professional certification question 113,https://www.examtopics.com/discussions/databricks/view/141545-exam-certified-data-engineer-professional-topic-1-question/,"A Delta Lake table in the Lakehouse named customer_churn_params is used in churn prediction by the machine learning team. The table contains information about customers derived from a number of upstream sources. Currently, the data engineering team populates this table nightly by overwriting the table with the current valid values derived from upstream data sources.Immediately after each update succeeds, the data engineering team would like to determine the difference between the new version and the previous version of the table.Given the current implementation, which method can be used?","['A.Execute a query to calculate the difference between the new version and the previous version using Delta Lake’s built-in versioning and lime travel functionality.', 'B.Parse the Delta Lake transaction log to identify all newly written data files.', 'C.Parse the Spark event logs to identify those rows that were updated, inserted, or deleted.', 'D.Execute DESCRIBE HISTORY customer_churn_params to obtain the full operation metrics for the update, including a log of all records that have been added or modified.', 'E.Use Delta Lake’s change data feed to identify those records that have been updated, inserted, or deleted.']",['A.Execute a query to calculate the difference between the new version and the previous version using Delta Lake’s built-in versioning and lime travel functionality.'],[],
114,examtopics databricks data engineer professional certification question 114,https://www.examtopics.com/discussions/databricks/view/141546-exam-certified-data-engineer-professional-topic-1-question/,"A data team’s Structured Streaming job is configured to calculate running aggregates for item sales to update a downstream marketing dashboard. The marketing team has introduced a new promotion, and they would like to add a new field to track the number of times this promotion code is used for each item. A junior data engineer suggests updating the existing query as follows. Note that proposed changes are in bold.Original query:Proposed query:Which step must also be completed to put the proposed query into production?","['A.Specify a new checkpointLocation', ""B.Remove .option('mergeSchema', 'true') from the streaming write"", 'C.Increase the shuffle partitions to account for additional aggregates', 'D.Run REFRESH TABLE delta.‛/item_agg‛']",['A.Specify a new checkpointLocation'],"['https://img.examtopics.com/certified-data-engineer-professional/image49.png', 'https://img.examtopics.com/certified-data-engineer-professional/image50.png']",
115,examtopics databricks data engineer professional certification question 115,https://www.examtopics.com/discussions/databricks/view/141547-exam-certified-data-engineer-professional-topic-1-question/,"When using CLI or REST API to get results from jobs with multiple tasks, which statement correctly describes the response structure?","['A.Each run of a job will have a unique job_id; all tasks within this job will have a unique job_id', 'B.Each run of a job will have a unique job_id; all tasks within this job will have a unique task_id', 'C.Each run of a job will have a unique orchestration_id; all tasks within this job will have a unique run_id', 'D.Each run of a job will have a unique run_id; all tasks within this job will have a unique task_id', 'E.Each run of a job will have a unique run_id; all tasks within this job will also have a unique run_id']",['E.Each run of a job will have a unique run_id; all tasks within this job will also have a unique run_id'],[],
116,examtopics databricks data engineer professional certification question 116,https://www.examtopics.com/discussions/databricks/view/141548-exam-certified-data-engineer-professional-topic-1-question/,"The data engineering team is configuring environments for development, testing, and production before beginning migration on a new data pipeline. The team requires extensive testing on both the code and data resulting from code execution, and the team wants to develop and test against data as similar to production data as possible.A junior data engineer suggests that production data can be mounted to the development and testing environments, allowing pre-production code to execute against production data. Because all users have admin privileges in the development environment, the junior data engineer has offered to configure permissions and mount this data for the team.Which statement captures best practices for this situation?","['A.All development, testing, and production code and data should exist in a single, unified workspace; creating separate environments for testing and development complicates administrative overhead.', 'B.In environments where interactive code will be executed, production data should only be accessible with read permissions; creating isolated databases for each environment further reduces risks.', 'C.As long as code in the development environment declares USE dev_db at the top of each notebook, there is no possibility of inadvertently committing changes back to production data sources.', 'D.Because Delta Lake versions all data and supports time travel, it is not possible for user error or malicious actors to permanently delete production data; as such, it is generally safe to mount production data anywhere.', 'E.Because access to production data will always be verified using passthrough credentials, it is safe to mount data to any Databricks development environment.']","['B.In environments where interactive code will be executed, production data should only be accessible with read permissions; creating isolated databases for each environment further reduces risks.']",[],
117,examtopics databricks data engineer professional certification question 117,https://www.examtopics.com/discussions/databricks/view/141951-exam-certified-data-engineer-professional-topic-1-question/,"A data engineer, User A, has promoted a pipeline to production by using the REST API to programmatically create several jobs. A DevOps engineer, User B, has configured an external orchestration tool to trigger job runs through the REST API. Both users authorized the REST API calls using their personal access tokens.A workspace admin, User C, inherits responsibility for managing this pipeline. User C uses the Databricks Jobs UI to take ""Owner"" privileges of each job. Jobs continue to be triggered using the credentials and tooling configured by User B.An application has been configured to collect and parse run information returned by the REST API. Which statement describes the value returned in the creator_user_name field?","['A.Once User C takes ""Owner"" privileges, their email address will appear in this field; prior to this, User A’s email address will appear in this field.', 'B.User B’s email address will always appear in this field, as their credentials are always used to trigger the run.', 'C.User A’s email address will always appear in this field, as they still own the underlying notebooks.', 'D.Once User C takes ""Owner"" privileges, their email address will appear in this field; prior to this, User B’s email address will appear in this field.', 'E.User C will only ever appear in this field if they manually trigger the job, otherwise it will indicate User B.']","['B.User B’s email address will always appear in this field, as their credentials are always used to trigger the run.']",[],
118,examtopics databricks data engineer professional certification question 118,https://www.examtopics.com/discussions/databricks/view/146882-exam-certified-data-engineer-professional-topic-1-question/,A member of the data engineering team has submitted a short notebook that they wish to schedule as part of a larger data pipeline. Assume that the commands provided below produce the logically correct results when run as presented.Which command should be removed from the notebook before scheduling it as a job?,"['A.Cmd 2', 'B.Cmd 3', 'C.Cmd 4', 'D.Cmd 5']",['D.Cmd 5'],['https://img.examtopics.com/certified-data-engineer-professional/image51.png'],
119,examtopics databricks data engineer professional certification question 119,https://www.examtopics.com/discussions/databricks/view/144252-exam-certified-data-engineer-professional-topic-1-question/,Which statement regarding Spark configuration on the Databricks platform is true?,"['A.The Databricks REST API can be used to modify the Spark configuration properties for an interactive cluster without interrupting jobs currently running on the cluster.', 'B.Spark configurations set within a notebook will affect all SparkSessions attached to the same interactive cluster.', 'C.When the same Spark configuration property is set for an interactive cluster and a notebook attached to that cluster, the notebook setting will always be ignored.', 'D.Spark configuration properties set for an interactive cluster with the Clusters UI will impact all notebooks attached to that cluster.']",['D.Spark configuration properties set for an interactive cluster with the Clusters UI will impact all notebooks attached to that cluster.'],[],
120,examtopics databricks data engineer professional certification question 120,https://www.examtopics.com/discussions/databricks/view/149442-exam-certified-data-engineer-professional-topic-1-question/,"The business reporting team requires that data for their dashboards be updated every hour. The total processing time for the pipeline that extracts, transforms, and loads the data for their pipeline runs in 10 minutes.Assuming normal operating conditions, which configuration will meet their service-level agreement requirements with the lowest cost?","['A.Configure a job that executes every time new data lands in a given directory', 'B.Schedule a job to execute the pipeline once an hour on a new job cluster', 'C.Schedule a Structured Streaming job with a trigger interval of 60 minutes', 'D.Schedule a job to execute the pipeline once an hour on a dedicated interactive cluster']",['B.Schedule a job to execute the pipeline once an hour on a new job cluster'],[],
121,examtopics databricks data engineer professional certification question 121,https://www.examtopics.com/discussions/databricks/view/141583-exam-certified-data-engineer-professional-topic-1-question/,A Databricks SQL dashboard has been configured to monitor the total number of records present in a collection of Delta Lake tables using the following query pattern:SELECT COUNT (*) FROM table -Which of the following describes how results are generated each time the dashboard is updated?,"['A.The total count of rows is calculated by scanning all data files', 'B.The total count of rows will be returned from cached results unless REFRESH is run', 'C.The total count of records is calculated from the Delta transaction logs', 'D.The total count of records is calculated from the parquet file metadata']",['C.The total count of records is calculated from the Delta transaction logs'],[],
122,examtopics databricks data engineer professional certification question 122,https://www.examtopics.com/discussions/databricks/view/141584-exam-certified-data-engineer-professional-topic-1-question/,"A Delta Lake table was created with the below query:Consider the following query:DROP TABLE prod.sales_by_store -If this statement is executed by a workspace admin, which result will occur?","['A.Data will be marked as deleted but still recoverable with Time Travel.', 'B.The table will be removed from the catalog but the data will remain in storage.', 'C.The table will be removed from the catalog and the data will be deleted.', 'D.An error will occur because Delta Lake prevents the deletion of production data.']",['C.The table will be removed from the catalog and the data will be deleted.'],['https://img.examtopics.com/certified-data-engineer-professional/image52.png'],
123,examtopics databricks data engineer professional certification question 123,https://www.examtopics.com/discussions/databricks/view/142864-exam-certified-data-engineer-professional-topic-1-question/,"A developer has successfully configured their credentials for Databricks Repos and cloned a remote Git repository. They do not have privileges to make changes to the main branch, which is the only branch currently visible in their workspace.Which approach allows this user to share their code updates without the risk of overwriting the work of their teammates?","['A.Use Repos to create a new branch, commit all changes, and push changes to the remote Git repository.', 'B.Use Repos to create a fork of the remote repository, commit all changes, and make a pull request on the source repository.', 'C.Use Repos to pull changes from the remote Git repository; commit and push changes to a branch that appeared as changes were pulled.', 'D.Use Repos to merge all differences and make a pull request back to the remote repository.']","['A.Use Repos to create a new branch, commit all changes, and push changes to the remote Git repository.']",[],
124,examtopics databricks data engineer professional certification question 124,https://www.examtopics.com/discussions/databricks/view/141900-exam-certified-data-engineer-professional-topic-1-question/,"The security team is exploring whether or not the Databricks secrets module can be leveraged for connecting to an external database.After testing the code with all Python variables being defined with strings, they upload the password to the secrets module and configure the correct permissions for the currently active user. They then modify their code to the following (leaving all other variables unchanged).Which statement describes what will happen when the above code is executed?","['A.The connection to the external table will succeed; the string ""REDACTED"" will be printed.', 'B.An interactive input box will appear in the notebook; if the right password is provided, the connection will succeed and the encoded password will be saved to DBFS.', 'C.An interactive input box will appear in the notebook; if the right password is provided, the connection will succeed and the password will be printed in plain text.', 'D.The connection to the external table will succeed; the string value of password will be printed in plain text.']","['A.The connection to the external table will succeed; the string ""REDACTED"" will be printed.']",['https://img.examtopics.com/certified-data-engineer-professional/image53.png'],
125,examtopics databricks data engineer professional certification question 125,https://www.examtopics.com/discussions/databricks/view/141740-exam-certified-data-engineer-professional-topic-1-question/,"The data science team has created and logged a production model using MLflow. The model accepts a list of column names and returns a new column of type DOUBLE.The following code correctly imports the production model, loads the customers table containing the customer_id key column into a DataFrame, and defines the feature columns needed for the model.Which code block will output a DataFrame with the schema ""customer_id LONG, predictions DOUBLE""?","['A.df.map(lambda x:model(x[columns])).select(""customer_id, predictions"")', 'B.df.select(""customer_id"",model(*columns).alias(""predictions""))', 'C.model.predict(df, columns)', 'D.df.apply(model, columns).select(""customer_id, predictions"")']","['B.df.select(""customer_id"",model(*columns).alias(""predictions""))']",['https://img.examtopics.com/certified-data-engineer-professional/image54.png'],
126,examtopics databricks data engineer professional certification question 126,https://www.examtopics.com/discussions/databricks/view/142366-exam-certified-data-engineer-professional-topic-1-question/,"A junior member of the data engineering team is exploring the language interoperability of Databricks notebooks. The intended outcome of the below code is to register a view of all sales that occurred in countries on the continent of Africa that appear in the geo_lookup table.Before executing the code, running SHOW TABLES on the current database indicates the database contains only two tables: geo_lookup and sales.What will be the outcome of executing these command cells m order m an interactive notebook?","['A.Both commands will succeed. Executing SHOW TABLES will show that countries_af and sales_af have been registered as views.', 'B.Cmd 1 will succeed. Cmd 2 will search all accessible databases for a table or view named countries_af: if this entity exists, Cmd 2 will succeed.', 'C.Cmd 1 will succeed and Cmd 2 will fail. countries_af will be a Python variable representing a PySpark DataFrame.', 'D.Cmd 1 will succeed and Cmd 2 will fail. countries_af will be a Python variable containing a list of strings.']",['D.Cmd 1 will succeed and Cmd 2 will fail. countries_af will be a Python variable containing a list of strings.'],['https://img.examtopics.com/certified-data-engineer-professional/image55.png'],
127,examtopics databricks data engineer professional certification question 127,https://www.examtopics.com/discussions/databricks/view/149833-exam-certified-data-engineer-professional-topic-1-question/,"The data science team has requested assistance in accelerating queries on free-form text from user reviews. The data is currently stored in Parquet with the below schema:item_id INT, user_id INT, review_id INT, rating FLOAT, review STRINGThe review column contains the full text of the review left by the user. Specifically, the data science team is looking to identify if any of 30 key words exist in this field.A junior data engineer suggests converting this data to Delta Lake will improve query performance.Which response to the junior data engineer’s suggestion is correct?","['A.Delta Lake statistics are not optimized for free text fields with high cardinality.', 'B.Delta Lake statistics are only collected on the first 4 columns in a table.', 'C.ZORDER ON review will need to be run to see performance gains.', 'D.The Delta log creates a term matrix for free text fields to support selective filtering.']",['A.Delta Lake statistics are not optimized for free text fields with high cardinality.'],[],
128,examtopics databricks data engineer professional certification question 128,https://www.examtopics.com/discussions/databricks/view/142967-exam-certified-data-engineer-professional-topic-1-question/,"The data engineering team has configured a job to process customer requests to be forgotten (have their data deleted). All user data that needs to be deleted is stored in Delta Lake tables using default table settings.The team has decided to process all deletions from the previous week as a batch job at 1am each Sunday. The total duration of this job is less than one hour. Every Monday at 3am, a batch job executes a series of VACUUM commands on all Delta Lake tables throughout the organization.The compliance officer has recently learned about Delta Lake's time travel functionality. They are concerned that this might allow continued access to deleted data.Assuming all delete logic is correctly implemented, which statement correctly addresses this concern?","['A.Because the VACUUM command permanently deletes all files containing deleted records, deleted records may be accessible with time travel for around 24 hours.', 'B.Because the default data retention threshold is 24 hours, data files containing deleted records will be retained until the VACUUM job is run the following day.', 'C.Because the default data retention threshold is 7 days, data files containing deleted records will be retained until the VACUUM job is run 8 days later.', ""D.Because Delta Lake's delete statements have ACID guarantees, deleted records will be permanently purged from all storage systems as soon as a delete job completes.""]","['C.Because the default data retention threshold is 7 days, data files containing deleted records will be retained until the VACUUM job is run 8 days later.']",[],
129,examtopics databricks data engineer professional certification question 129,https://www.examtopics.com/discussions/databricks/view/144256-exam-certified-data-engineer-professional-topic-1-question/,"Assuming that the Databricks CLI has been installed and configured correctly, which Databricks CLI command can be used to upload a custom Python Wheel to object storage mounted with the DBFS for use with a production job?","['A.configure', 'B.fs', 'C.workspace', 'D.libraries']",['B.fs'],[],
130,examtopics databricks data engineer professional certification question 130,https://www.examtopics.com/discussions/databricks/view/141552-exam-certified-data-engineer-professional-topic-1-question/,"The following table consists of items found in user carts within an e-commerce website.The following MERGE statement is used to update this table using an updates view, with schema evolution enabled on this table.How would the following update be handled?","['A.The update throws an error because changes to existing columns in the target schema are not supported.', 'B.The new nested Field is added to the target schema, and dynamically read as NULL for existing unmatched records.', 'C.The update is moved to a separate ""rescued"" column because it is missing a column expected in the target schema.', 'D.The new nested field is added to the target schema, and files underlying existing records are updated to include NULL values for the new field.']","['B.The new nested Field is added to the target schema, and dynamically read as NULL for existing unmatched records.']","['https://img.examtopics.com/certified-data-engineer-professional/image56.png', 'https://img.examtopics.com/certified-data-engineer-professional/image57.png', 'https://img.examtopics.com/certified-data-engineer-professional/image58.png']",
131,examtopics databricks data engineer professional certification question 131,https://www.examtopics.com/discussions/databricks/view/146476-exam-certified-data-engineer-associate-topic-1-question-131/,A data engineer has been using a Databricks SQL dashboard to monitor the cleanliness of the input data to an ELT job. The ELT job has its Databricks SQL query that returns the number of input records containing unexpected NULL values. The data engineer wants their entire team to be notified via a messaging webhook whenever this value reaches 100.Which approach can the data engineer use to notify their entire team via a messaging webhook whenever the number of NULL values reaches 100?,"['A.They can set up an Alert with a custom template.', 'B.They can set up an Alert with a new email alert destination.', 'C.They can set up an Alert with a new webhook alert destination.', 'D.They can set up an Alert with one-time notifications.']",['C.They can set up an Alert with a new webhook alert destination.'],[],
132,examtopics databricks data engineer professional certification question 132,https://www.examtopics.com/discussions/databricks/view/141744-exam-certified-data-engineer-professional-topic-1-question/,"An hourly batch job is configured to ingest data files from a cloud object storage container where each batch represent all records produced by the source system in a given hour. The batch job to process these records into the Lakehouse is sufficiently delayed to ensure no late-arriving data is missed. The user_id field represents a unique key for the data, which has the following schema:user_id BIGINT, username STRING, user_utc STRING, user_region STRING, last_login BIGINT, auto_pay BOOLEAN, last_updated BIGINTNew records are all ingested into a table named account_history which maintains a full record of all data in the same schema as the source. The next table in the system is named account_current and is implemented as a Type 1 table representing the most recent value for each unique user_id.Which implementation can be used to efficiently update the described account_current table as part of each hourly batch job assuming there are millions of user accounts and tens of thousands of records processed hourly?","['A.Filter records in account_history using the last_updated field and the most recent hour processed, making sure to deduplicate on username; write a merge statement to update or insert the most recent value for each username.', 'B.Use Auto Loader to subscribe to new files in the account_history directory; configure a Structured Streaming trigger available job to batch update newly detected files into the account_current table.', 'C.Overwrite the account_current table with each batch using the results of a query against the account_history table grouping by user_id and filtering for the max value of last_updated.', 'D.Filter records in account_history using the last_updated field and the most recent hour processed, as well as the max last_login by user_id write a merge statement to update or insert the most recent value for each user_id.']","['D.Filter records in account_history using the last_updated field and the most recent hour processed, as well as the max last_login by user_id write a merge statement to update or insert the most recent value for each user_id.']",[],
133,examtopics databricks data engineer professional certification question 133,https://www.examtopics.com/discussions/databricks/view/141553-exam-certified-data-engineer-professional-topic-1-question/,"The business intelligence team has a dashboard configured to track various summary metrics for retail stores. This includes total sales for the previous day alongside totals and averages for a variety of time periods. The fields required to populate this dashboard have the following schema:For demand forecasting, the Lakehouse contains a validated table of all itemized sales updated incrementally in near real-time. This table, named products_per_order, includes the following fields:Because reporting on long-term sales trends is less volatile, analysts using the new dashboard only require data to be refreshed once daily. Because the dashboard will be queried interactively by many users throughout a normal business day, it should return results quickly and reduce total compute associated with each materialization.Which solution meets the expectations of the end users while controlling and limiting possible costs?","['A.Populate the dashboard by configuring a nightly batch job to save the required values as a table overwritten with each update.', 'B.Use Structured Streaming to configure a live dashboard against the products_per_order table within a Databricks notebook.', 'C.Define a view against the products_per_order table and define the dashboard against this view.', 'D.Use the Delta Cache to persist the products_per_order table in memory to quickly update the dashboard with each query.']",['A.Populate the dashboard by configuring a nightly batch job to save the required values as a table overwritten with each update.'],"['https://img.examtopics.com/certified-data-engineer-professional/image59.png', 'https://img.examtopics.com/certified-data-engineer-professional/image60.png']",
134,examtopics databricks data engineer professional certification question 134,https://www.examtopics.com/discussions/databricks/view/149836-exam-certified-data-engineer-professional-topic-1-question/,"A Delta lake table with CDF enabled table in the Lakehouse named customer_churn_params is used in churn prediction by the machine learning team. The table contains information about customers derived from a number of upstream sources. Currently, the data engineering team populates this table nightly by overwriting the table with the current valid values derived from upstream data sources.The churn prediction model used by the ML team is fairly stable in production. The team is only interested in making predictions on records that have changed in the past 24 hours.Which approach would simplify the identification of these changed records?","['A.Apply the churn model to all rows in the customer_churn_params table, but implement logic to perform an upsert into the predictions table that ignores rows where predictions have not changed.', 'B.Convert the batch job to a Structured Streaming job using the complete output mode; configure a Structured Streaming job to read from the customer_churn_params table and incrementally predict against the churn model.', 'C.Replace the current overwrite logic with a merge statement to modify only those records that have changed; write logic to make predictions on the changed records identified by the change data feed.', 'D.Modify the overwrite logic to include a field populated by calling spark.sql.functions.current_timestamp() as data are being written; use this field to identify records written on a particular date.']",['C.Replace the current overwrite logic with a merge statement to modify only those records that have changed; write logic to make predictions on the changed records identified by the change data feed.'],[],
135,examtopics databricks data engineer professional certification question 135,https://www.examtopics.com/discussions/databricks/view/141585-exam-certified-data-engineer-professional-topic-1-question/,A view is registered with the following code:Both users and orders are Delta Lake tables.Which statement describes the results of querying recent_orders?,"['A.All logic will execute when the view is defined and store the result of joining tables to the DBFS; this stored data will be returned when the view is queried.', 'B.Results will be computed and cached when the view is defined; these cached results will incrementally update as new records are inserted into source tables.', 'C.All logic will execute at query time and return the result of joining the valid versions of the source tables at the time the query finishes.', 'D.All logic will execute at query time and return the result of joining the valid versions of the source tables at the time the query began.']",['D.All logic will execute at query time and return the result of joining the valid versions of the source tables at the time the query began.'],['https://img.examtopics.com/certified-data-engineer-professional/image61.png'],
136,examtopics databricks data engineer professional certification question 136,https://www.examtopics.com/discussions/databricks/view/141745-exam-certified-data-engineer-professional-topic-1-question/,"A data ingestion task requires a one-TB JSON dataset to be written out to Parquet with a target part-file size of 512 MB. Because Parquet is being used instead of Delta Lake, built-in file-sizing features such as Auto-Optimize & Auto-Compaction cannot be used.Which strategy will yield the best performance without shuffling data?","['A.Set spark.sql.files.maxPartitionBytes to 512 MB, ingest the data, execute the narrow transformations, and then write to parquet.', 'B.Set spark.sql.shuffle.partitions to 2,048 partitions (1TB*1024*1024/512), ingest the data, execute the narrow transformations, optimize the data by sorting it (which automatically repartitions the data), and then write to parquet.', 'C.Set spark.sql.adaptive.advisoryPartitionSizeInBytes to 512 MB bytes, ingest the data, execute the narrow transformations, coalesce to 2,048 partitions (1TB*1024*1024/512), and then write to parquet.', 'D.Ingest the data, execute the narrow transformations, repartition to 2,048 partitions (1TB* 1024*1024/512), and then write to parquet.']","['A.Set spark.sql.files.maxPartitionBytes to 512 MB, ingest the data, execute the narrow transformations, and then write to parquet.']",[],
137,examtopics databricks data engineer professional certification question 137,https://www.examtopics.com/discussions/databricks/view/149798-exam-certified-data-engineer-associate-topic-1-question-137/,Identify how the count_if function and the count where x is null can be usedConsider a table random_values with below data.What would be the output of below query?select count_if(col > 1) as count_a. count(*) as count_b.count(col1) as count_c from random_values col1012NULL -23,"['A.3 6 5', 'B.4 6 5', 'C.3 6 6', 'D.4 6 6']",['A.3 6 5'],[],
138,examtopics databricks data engineer professional certification question 138,https://www.examtopics.com/discussions/databricks/view/149854-exam-certified-data-engineer-professional-topic-1-question/,"A junior data engineer has been asked to develop a streaming data pipeline with a grouped aggregation using DataFrame df. The pipeline needs to calculate the average humidity and average temperature for each non-overlapping five-minute interval. Events are recorded once per minute per device.Streaming DataFrame df has the following schema:""device_id INT, event_time TIMESTAMP, temp FLOAT, humidity FLOAT""Code block:Which line of code correctly fills in the blank within the code block to complete this task?","['A.to_interval(""event_time"", ""5 minutes"").alias(""time"")', 'B.window(""event_time"", ""5 minutes"").alias(""time"")', 'C.""event_time""', 'D.lag(""event_time"", ""10 minutes"").alias(""time"")']","['B.window(""event_time"", ""5 minutes"").alias(""time"")']",['https://img.examtopics.com/certified-data-engineer-professional/image62.png'],
139,examtopics databricks data engineer professional certification question 139,https://www.examtopics.com/discussions/databricks/view/142392-exam-certified-data-engineer-professional-topic-1-question/,"A Structured Streaming job deployed to production has been resulting in higher than expected cloud storage costs. At present, during normal execution, each microbatch of data is processed in less than 3s; at least 12 times per minute, a microbatch is processed that contains 0 records. The streaming write was configured using the default trigger settings. The production job is currently scheduled alongside many other Databricks jobs in a workspace with instance pools provisioned to reduce start-up time for jobs with batch execution.Holding all other variables constant and assuming records need to be processed in less than 10 minutes, which adjustment will meet the requirement?","['A.Set the trigger interval to 3 seconds; the default trigger interval is consuming too many records per batch, resulting in spill to disk that can increase volume costs.', 'B.Use the trigger once option and configure a Databricks job to execute the query every 10 minutes; this approach minimizes costs for both compute and storage.', 'C.Set the trigger interval to 10 minutes; each batch calls APIs in the source storage account, so decreasing trigger frequency to maximum allowable threshold should minimize this cost.', 'D.Set the trigger interval to 500 milliseconds; setting a small but non-zero trigger interval ensures that the source is not queried too frequently.']","['C.Set the trigger interval to 10 minutes; each batch calls APIs in the source storage account, so decreasing trigger frequency to maximum allowable threshold should minimize this cost.']",[],
140,examtopics databricks data engineer professional certification question 140,https://www.examtopics.com/discussions/databricks/view/144257-exam-certified-data-engineer-professional-topic-1-question/,Which statement describes Delta Lake optimized writes?,"['A.Before a Jobs cluster terminates, OPTIMIZE is executed on all tables modified during the most recent job.', 'B.An asynchronous job runs after the write completes to detect if files could be further compacted; if yes, an OPTIMIZE job is executed toward a default of 1 GB.', 'C.A shuffle occurs prior to writing to try to group similar data together resulting in fewer files instead of each executor writing multiple files based on directory partitions.', 'D.Optimized writes use logical partitions instead of directory partitions; because partition boundaries are only represented in metadata, fewer small files are written.']",['C.A shuffle occurs prior to writing to try to group similar data together resulting in fewer files instead of each executor writing multiple files based on directory partitions.'],[],
141,examtopics databricks data engineer professional certification question 141,https://www.examtopics.com/discussions/databricks/view/144258-exam-certified-data-engineer-professional-topic-1-question/,Which statement characterizes the general programming model used by Spark Structured Streaming?,"['A.Structured Streaming leverages the parallel processing of GPUs to achieve highly parallel data throughput.', 'B.Structured Streaming is implemented as a messaging bus and is derived from Apache Kafka.', 'C.Structured Streaming relies on a distributed network of nodes that hold incremental state values for cached stages.', 'D.Structured Streaming models new data arriving in a data stream as new rows appended to an unbounded table.']",['D.Structured Streaming models new data arriving in a data stream as new rows appended to an unbounded table.'],[],
142,examtopics databricks data engineer professional certification question 142,https://www.examtopics.com/discussions/databricks/view/144259-exam-certified-data-engineer-professional-topic-1-question/,Which configuration parameter directly affects the size of a spark-partition upon ingestion of data into Spark?,"['A.spark.sql.files.maxPartitionBytes', 'B.spark.sql.autoBroadcastJoinThreshold', 'C.spark.sql.adaptive.advisoryPartitionSizeInBytes', 'D.spark.sql.adaptive.coalescePartitions.minPartitionNum']",['A.spark.sql.files.maxPartitionBytes'],[],
143,examtopics databricks data engineer professional certification question 143,https://www.examtopics.com/discussions/databricks/view/144260-exam-certified-data-engineer-professional-topic-1-question/,"A Spark job is taking longer than expected. Using the Spark UI, a data engineer notes that the Min, Median, and Max Durations for tasks in a particular stage show the minimum and median time to complete a task as roughly the same, but the max duration for a task to be roughly 100 times as long as the minimum.Which situation is causing increased duration of the overall job?","['A.Task queueing resulting from improper thread pool assignment.', 'B.Spill resulting from attached volume storage being too small.', 'C.Network latency due to some cluster nodes being in different regions from the source data', 'D.Skew caused by more data being assigned to a subset of spark-partitions.']",['D.Skew caused by more data being assigned to a subset of spark-partitions.'],[],
144,examtopics databricks data engineer professional certification question 144,https://www.examtopics.com/discussions/databricks/view/149859-exam-certified-data-engineer-professional-topic-1-question/,"Each configuration below is identical to the extent that each cluster has 400 GB total of RAM, 160 total cores and only one Executor per VM.Given an extremely long-running job for which completion must be guaranteed, which cluster configuration will be able to guarantee completion of the job in light of one or more VM failures?","['A.• Total VMs: 8• 50 GB per Executor• 20 Cores / Executor', 'B.• Total VMs: 16• 25 GB per Executor• 10 Cores / Executor', 'C.• Total VMs: 1• 400 GB per Executor• 160 Cores/Executor', 'D.• Total VMs: 4• 100 GB per Executor• 40 Cores / Executor']",['B.• Total VMs: 16• 25 GB per Executor• 10 Cores / Executor'],[],
145,examtopics databricks data engineer professional certification question 145,https://www.examtopics.com/discussions/databricks/view/146974-exam-certified-data-engineer-professional-topic-1-question/,"A task orchestrator has been configured to run two hourly tasks. First, an outside system writes Parquet data to a directory mounted at /mnt/raw_orders/. After this data is written, a Databricks job containing the following code is executed:Assume that the fields customer_id and order_id serve as a composite key to uniquely identify each order, and that the time field indicates when the record was queued in the source system.If the upstream system is known to occasionally enqueue duplicate entries for a single order hours apart, which statement is correct?","['A.Duplicate records enqueued more than 2 hours apart may be retained and the orders table may contain duplicate records with the same customer_id and order_id.', 'B.All records will be held in the state store for 2 hours before being deduplicated and committed to the orders table.', 'C.The orders table will contain only the most recent 2 hours of records and no duplicates will be present.', 'D.The orders table will not contain duplicates, but records arriving more than 2 hours late will be ignored and missing from the table.']",['A.Duplicate records enqueued more than 2 hours apart may be retained and the orders table may contain duplicate records with the same customer_id and order_id.'],['https://img.examtopics.com/certified-data-engineer-professional/image63.png'],
146,examtopics databricks data engineer professional certification question 146,https://www.examtopics.com/discussions/databricks/view/149865-exam-certified-data-engineer-professional-topic-1-question/,"A data engineer is configuring a pipeline that will potentially see late-arriving, duplicate records.In addition to de-duplicating records within the batch, which of the following approaches allows the data engineer to deduplicate data against previously processed records as it is inserted into a Delta table?","['A.Rely on Delta Lake schema enforcement to prevent duplicate records.', 'B.VACUUM the Delta table after each batch completes.', 'C.Perform an insert-only merge with a matching condition on a unique key.', 'D.Perform a full outer join on a unique key and overwrite existing data.']",['C.Perform an insert-only merge with a matching condition on a unique key.'],[],
147,examtopics databricks data engineer professional certification question 147,https://www.examtopics.com/discussions/databricks/view/147917-exam-certified-data-engineer-professional-topic-1-question/,A junior data engineer seeks to leverage Delta Lake's Change Data Feed functionality to create a Type 1 table representing all of the values that have ever been valid for all rows in a bronze table created with the property delta.enableChangeDataFeed = true. They plan to execute the following code as a daily job:Which statement describes the execution and results of running the above query multiple times?,"['A.Each time the job is executed, newly updated records will be merged into the target table, overwriting previous values with the same primary keys.', 'B.Each time the job is executed, the entire available history of inserted or updated records will be appended to the target table, resulting in many duplicate entries.', 'C.Each time the job is executed, only those records that have been inserted or updated since the last execution will be appended to the target table, giving the desired result.', 'D.Each time the job is executed, the differences between the original and current versions are calculated; this may result in duplicate entries for some records.']","['B.Each time the job is executed, the entire available history of inserted or updated records will be appended to the target table, resulting in many duplicate entries.']",['https://img.examtopics.com/certified-data-engineer-professional/image64.png'],
148,examtopics databricks data engineer professional certification question 148,https://www.examtopics.com/discussions/databricks/view/141556-exam-certified-data-engineer-professional-topic-1-question/,"A DLT pipeline includes the following streaming tables:•	raw_iot ingests raw device measurement data from a heart rate tracking device.•	bpm_stats incrementally computes user statistics based on BPM measurements from raw_iot.How can the data engineer configure this pipeline to be able to retain manually deleted or updated records in the raw_iot table, while recomputing the downstream table bpm_stats table when a pipeline update is run?","['A.Set the pipelines.reset.allowed property to false on raw_iot', 'B.Set the skipChangeCommits flag to true on raw_iot', 'C.Set the pipelines.reset.allowed property to false on bpm_stats', 'D.Set the skipChangeCommits flag to true on bpm_stats']",['A.Set the pipelines.reset.allowed property to false on raw_iot'],[],
149,examtopics databricks data engineer professional certification question 149,https://www.examtopics.com/discussions/databricks/view/149871-exam-certified-data-engineer-professional-topic-1-question/,"A data pipeline uses Structured Streaming to ingest data from Apache Kafka to Delta Lake. Data is being stored in a bronze table, and includes the Kafka-generated timestamp, key, and value. Three months after the pipeline is deployed, the data engineering team has noticed some latency issues during certain times of the day.A senior data engineer updates the Delta Table's schema and ingestion logic to include the current timestamp (as recorded by Apache Spark) as well as the Kafka topic and partition. The team plans to use these additional metadata fields to diagnose the transient processing delays.Which limitation will the team face while diagnosing this problem?","['A.New fields will not be computed for historic records.', 'B.Spark cannot capture the topic and partition fields from a Kafka source.', 'C.Updating the table schema requires a default value provided for each field added.', 'D.Updating the table schema will invalidate the Delta transaction log metadata.']",['A.New fields will not be computed for historic records.'],[],
150,examtopics databricks data engineer professional certification question 150,https://www.examtopics.com/discussions/databricks/view/141557-exam-certified-data-engineer-professional-topic-1-question/,A nightly job ingests data into a Delta Lake table using the following code:The next step in the pipeline requires a function that returns an object that can be used to manipulate new records that have not yet been processed to the next table in the pipeline.Which code snippet completes this function definition?def new_records():,"['A.return spark.readStream.table(""bronze"")', 'B.return spark.read.option(""readChangeFeed"", ""true"").table (""bronze"")', 'C.', 'D.']","['A.return spark.readStream.table(""bronze"")']",['https://img.examtopics.com/certified-data-engineer-professional/image65.png'],
