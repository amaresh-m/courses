question_num,search_query,url,Question,Options,Correct answer,Image URLs,Comments
151,examtopics databricks data engineer professional certification question 151,https://www.examtopics.com/discussions/databricks/view/142393-exam-certified-data-engineer-professional-topic-1-question/,"A junior data engineer is working to implement logic for a Lakehouse table named silver_device_recordings. The source data contains 100 unique fields in a highly nested JSON structure.The silver_device_recordings table will be used downstream to power several production monitoring dashboards and a production model. At present, 45 of the 100 fields are being used in at least one of these applications.The data engineer is trying to determine the best approach for dealing with schema declaration given the highly-nested structure of the data and the numerous fields.Which of the following accurately presents information about Delta Lake and Databricks that may impact their decision-making process?","['A.The Tungsten encoding used by Databricks is optimized for storing string data; newly-added native support for querying JSON strings means that string types are always most efficient.', 'B.Because Delta Lake uses Parquet for data storage, data types can be easily evolved by just modifying file footer information in place.', 'C.Schema inference and evolution on Databricks ensure that inferred types will always accurately match the data types used by downstream systems.', 'D.Because Databricks will infer schema using types that allow all observed data to be processed, setting types manually provides greater assurance of data quality enforcement.']","['D.Because Databricks will infer schema using types that allow all observed data to be processed, setting types manually provides greater assurance of data quality enforcement.']",[],
152,examtopics databricks data engineer professional certification question 152,https://www.examtopics.com/discussions/databricks/view/142394-exam-certified-data-engineer-professional-topic-1-question/,"The data engineering team maintains the following code:Assuming that this code produces logically correct results and the data in the source tables has been de-duplicated and validated, which statement describes what will occur when this code is executed?","['A.A batch job will update the enriched_itemized_orders_by_account table, replacing only those rows that have different values than the current version of the table, using accountID as the primary key.', 'B.The enriched_itemized_orders_by_account table will be overwritten using the current valid version of data in each of the three tables referenced in the join logic.', 'C.No computation will occur until enriched_itemized_orders_by_account is queried; upon query materialization, results will be calculated using the current valid version of data in each of the three tables referenced in the join logic.', 'D.An incremental job will detect if new rows have been written to any of the source tables; if new rows are detected, all results will be recalculated and used to overwrite the enriched_itemized_orders_by_account table.']",['B.The enriched_itemized_orders_by_account table will be overwritten using the current valid version of data in each of the three tables referenced in the join logic.'],['https://img.examtopics.com/certified-data-engineer-professional/image68.png'],
153,examtopics databricks data engineer professional certification question 153,https://www.examtopics.com/discussions/databricks/view/149874-exam-certified-data-engineer-professional-topic-1-question/,"The data engineering team is configuring environments for development, testing, and production before beginning migration on a new data pipeline. The team requires extensive testing on both the code and data resulting from code execution, and the team wants to develop and test against data as similar to production data as possible.A junior data engineer suggests that production data can be mounted to the development and testing environments, allowing pre-production code to execute against production data. Because all users have admin privileges in the development environment, the junior data engineer has offered to configure permissions and mount this data for the team.Which statement captures best practices for this situation?","['A.All development, testing, and production code and data should exist in a single, unified workspace; creating separate environments for testing and development complicates administrative overhead.', 'B.In environments where interactive code will be executed, production data should only be accessible with read permissions; creating isolated databases for each environment further reduces risks.', 'C.Because access to production data will always be verified using passthrough credentials, it is safe to mount data to any Databricks development environment.', 'D.Because Delta Lake versions all data and supports time travel, it is not possible for user error or malicious actors to permanently delete production data; as such, it is generally safe to mount production data anywhere.']","['B.In environments where interactive code will be executed, production data should only be accessible with read permissions; creating isolated databases for each environment further reduces risks.']",[],
154,examtopics databricks data engineer professional certification question 154,https://www.examtopics.com/discussions/databricks/view/149876-exam-certified-data-engineer-professional-topic-1-question/,The data architect has mandated that all tables in the Lakehouse should be configured as external Delta Lake tables.Which approach will ensure that this requirement is met?,"['A.Whenever a database is being created, make sure that the LOCATION keyword is used.', 'B.When the workspace is being configured, make sure that external cloud object storage has been mounted.', 'C.Whenever a table is being created, make sure that the LOCATION keyword is used.', 'D.When tables are created, make sure that the UNMANAGED keyword is used in the CREATE TABLE statement.']","['C.Whenever a table is being created, make sure that the LOCATION keyword is used.']",[],
155,examtopics databricks data engineer professional certification question 155,https://www.examtopics.com/discussions/databricks/view/147232-exam-certified-data-engineer-professional-topic-1-question/,"The marketing team is looking to share data in an aggregate table with the sales organization, but the field names used by the teams do not match, and a number of marketing-specific fields have not been approved for the sales org.Which of the following solutions addresses the situation while emphasizing simplicity?","['A.Create a view on the marketing table selecting only those fields approved for the sales team; alias the names of any fields that should be standardized to the sales naming conventions.', ""B.Create a new table with the required schema and use Delta Lake's DEEP CLONE functionality to sync up changes committed to one table to the corresponding table."", 'C.Use a CTAS statement to create a derivative table from the marketing table; configure a production job to propagate changes.', 'D.Add a parallel table write to the current production pipeline, updating a new sales table that varies as required from the marketing table.']",['A.Create a view on the marketing table selecting only those fields approved for the sales team; alias the names of any fields that should be standardized to the sales naming conventions.'],[],
156,examtopics databricks data engineer professional certification question 156,https://www.examtopics.com/discussions/databricks/view/144262-exam-certified-data-engineer-professional-topic-1-question/,"A Delta Lake table representing metadata about content posts from users has the following schema:user_id LONG, post_text STRING, post_id STRING, longitude FLOAT, latitude FLOAT, post_time TIMESTAMP, date DATEThis table is partitioned by the date column. A query is run with the following filter:longitude < 20 & longitude > -20Which statement describes how data will be filtered?","['A.Statistics in the Delta Log will be used to identify partitions that might Include files in the filtered range.', 'B.No file skipping will occur because the optimizer does not know the relationship between the partition column and the longitude.', 'C.The Delta Engine will scan the parquet file footers to identify each row that meets the filter criteria.', 'D.Statistics in the Delta Log will be used to identify data files that might include records in the filtered range.']",['D.Statistics in the Delta Log will be used to identify data files that might include records in the filtered range.'],[],
157,examtopics databricks data engineer professional certification question 157,https://www.examtopics.com/discussions/databricks/view/142396-exam-certified-data-engineer-professional-topic-1-question/,"A small company based in the United States has recently contracted a consulting firm in India to implement several new data engineering pipelines to power artificial intelligence applications. All the company's data is stored in regional cloud storage in the United States.The workspace administrator at the company is uncertain about where the Databricks workspace used by the contractors should be deployed.Assuming that all data governance considerations are accounted for, which statement accurately informs this decision?","['A.Databricks runs HDFS on cloud volume storage; as such, cloud virtual machines must be deployed in the region where the data is stored.', 'B.Databricks workspaces do not rely on any regional infrastructure; as such, the decision should be made based upon what is most convenient for the workspace administrator.', 'C.Cross-region reads and writes can incur significant costs and latency; whenever possible, compute should be deployed in the same region the data is stored.', 'D.Databricks notebooks send all executable code from the user’s browser to virtual machines over the open internet; whenever possible, choosing a workspace region near the end users is the most secure.']","['C.Cross-region reads and writes can incur significant costs and latency; whenever possible, compute should be deployed in the same region the data is stored.']",[],
158,examtopics databricks data engineer professional certification question 158,https://www.examtopics.com/discussions/databricks/view/144263-exam-certified-data-engineer-professional-topic-1-question/,"A CHECK constraint has been successfully added to the Delta table named activity_details using the following logic:A batch job is attempting to insert new records to the table, including a record where latitude = 45.50 and longitude = 212.67.Which statement describes the outcome of this batch insert?","['A.The write will insert all records except those that violate the table constraints; the violating records will be reported in a warning log.', 'B.The write will fail completely because of the constraint violation and no records will be inserted into the target table.', 'C.The write will insert all records except those that violate the table constraints; the violating records will be recorded to a quarantine table.', 'D.The write will include all records in the target table; any violations will be indicated in the boolean column named valid_coordinates.']",['B.The write will fail completely because of the constraint violation and no records will be inserted into the target table.'],['https://img.examtopics.com/certified-data-engineer-professional/image69.png'],
159,examtopics databricks data engineer professional certification question 159,https://www.examtopics.com/discussions/databricks/view/142397-exam-certified-data-engineer-professional-topic-1-question/,"A junior data engineer is migrating a workload from a relational database system to the Databricks Lakehouse. The source system uses a star schema, leveraging foreign key constraints and multi-table inserts to validate records on write.Which consideration will impact the decisions made by the engineer while migrating this workload?","['A.Databricks only allows foreign key constraints on hashed identifiers, which avoid collisions in highly-parallel writes.', 'B.Foreign keys must reference a primary key field; multi-table inserts must leverage Delta Lake’s upsert functionality.', 'C.Committing to multiple tables simultaneously requires taking out multiple table locks and can lead to a state of deadlock.', 'D.All Delta Lake transactions are ACID compliant against a single table, and Databricks does not enforce foreign key constraints.']","['D.All Delta Lake transactions are ACID compliant against a single table, and Databricks does not enforce foreign key constraints.']",[],
160,examtopics databricks data engineer professional certification question 160,https://www.examtopics.com/discussions/databricks/view/142398-exam-certified-data-engineer-professional-topic-1-question/,"A data architect has heard about Delta Lake’s built-in versioning and time travel capabilities. For auditing purposes, they have a requirement to maintain a full record of all valid street addresses as they appear in the customers table.The architect is interested in implementing a Type 1 table, overwriting existing records with new values and relying on Delta Lake time travel to support long-term auditing. A data engineer on the project feels that a Type 2 table will provide better performance and scalability.Which piece of information is critical to this decision?","['A.Data corruption can occur if a query fails in a partially completed state because Type 2 tables require setting multiple fields in a single update.', 'B.Shallow clones can be combined with Type 1 tables to accelerate historic queries for long-term versioning.', 'C.Delta Lake time travel cannot be used to query previous versions of these tables because Type 1 changes modify data files in place.', 'D.Delta Lake time travel does not scale well in cost or latency to provide a long-term versioning solution.']",['D.Delta Lake time travel does not scale well in cost or latency to provide a long-term versioning solution.'],[],
161,examtopics databricks data engineer professional certification question 161,https://www.examtopics.com/discussions/databricks/view/149882-exam-certified-data-engineer-professional-topic-1-question/,"A data engineer wants to join a stream of advertisement impressions (when an ad was shown) with another stream of user clicks on advertisements to correlate when impressions led to monetizable clicks.In the code below, Impressions is a streaming DataFrame with a watermark (""event_time"", ""10 minutes"")The data engineer notices the query slowing down significantly.Which solution would improve the performance?","['A.Joining on event time constraint: clickTime >= impressionTime AND clickTime <= impressionTime interval 1 hour', 'B.Joining on event time constraint: clickTime + 3 hours < impressionTime - 2 hours', 'C.Joining on event time constraint: clickTime == impressionTime using a leftOuter join', 'D.Joining on event time constraint: clickTime >= impressionTime - interval 3 hours and removing watermarks']",['A.Joining on event time constraint: clickTime >= impressionTime AND clickTime <= impressionTime interval 1 hour'],['https://img.examtopics.com/certified-data-engineer-professional/image70.png'],
162,examtopics databricks data engineer professional certification question 162,https://www.examtopics.com/discussions/databricks/view/141710-exam-certified-data-engineer-professional-topic-1-question/,"A junior data engineer has manually configured a series of jobs using the Databricks Jobs UI. Upon reviewing their work, the engineer realizes that they are listed as the ""Owner"" for each job. They attempt to transfer ""Owner"" privileges to the ""DevOps"" group, but cannot successfully accomplish this task.Which statement explains what is preventing this privilege transfer?","['A.Databricks jobs must have exactly one owner; ""Owner"" privileges cannot be assigned to a group.', 'B.The creator of a Databricks job will always have ""Owner"" privileges; this configuration cannot be changed.', 'C.Only workspace administrators can grant ""Owner"" privileges to a group.', 'D.A user can only transfer job ownership to a group if they are also a member of that group.']","['A.Databricks jobs must have exactly one owner; ""Owner"" privileges cannot be assigned to a group.']",[],
163,examtopics databricks data engineer professional certification question 163,https://www.examtopics.com/discussions/databricks/view/141558-exam-certified-data-engineer-professional-topic-1-question/,"A table named user_ltv is being used to create a view that will be used by data analysts on various teams. Users in the workspace are configured into groups, which are used for setting up data access using ACLs.The user_ltv table has the following schema:email STRING, age INT, ltv INTThe following view definition is executed:An analyst who is not a member of the auditing group executes the following query:SELECT * FROM user_ltv_no_minorsWhich statement describes the results returned by this query?","['A.All columns will be displayed normally for those records that have an age greater than 17; records not meeting this condition will be omitted.', 'B.All age values less than 18 will be returned as null values, all other columns will be returned with the values in user_ltv.', 'C.All values for the age column will be returned as null values, all other columns will be returned with the values in user_ltv.', 'D.All columns will be displayed normally for those records that have an age greater than 18; records not meeting this condition will be omitted.']",['A.All columns will be displayed normally for those records that have an age greater than 17; records not meeting this condition will be omitted.'],['https://img.examtopics.com/certified-data-engineer-professional/image71.png'],
164,examtopics databricks data engineer professional certification question 164,https://www.examtopics.com/discussions/databricks/view/141709-exam-certified-data-engineer-professional-topic-1-question/,"All records from an Apache Kafka producer are being ingested into a single Delta Lake table with the following schema:key BINARY, value BINARY, topic STRING, partition LONG, offset LONG, timestamp LONGThere are 5 unique topics being ingested. Only the ""registration"" topic contains Personal Identifiable Information (PII). The company wishes to restrict access to PII. The company also wishes to only retain records containing PII in this table for 14 days after initial ingestion. However, for non-PII information, it would like to retain these records indefinitely.Which solution meets the requirements?","[""A.All data should be deleted biweekly; Delta Lake's time travel functionality should be leveraged to maintain a history of non-PII information."", 'B.Data should be partitioned by the registration field, allowing ACLs and delete statements to be set for the PII directory.', 'C.Data should be partitioned by the topic field, allowing ACLs and delete statements to leverage partition boundaries.', 'D.Separate object storage containers should be specified based on the partition field, allowing isolation at the storage level.']","['C.Data should be partitioned by the topic field, allowing ACLs and delete statements to leverage partition boundaries.']",[],
165,examtopics databricks data engineer professional certification question 165,https://www.examtopics.com/discussions/databricks/view/141708-exam-certified-data-engineer-professional-topic-1-question/,"The data governance team is reviewing code used for deleting records for compliance with GDPR. The following logic has been implemented to propagate delete requests from the user_lookup table to the user_aggregates table.Assuming that user_id is a unique identifying key and that all users that have requested deletion have been removed from the user_lookup table, which statement describes whether successfully executing the above logic guarantees that the records to be deleted from the user_aggregates table are no longer accessible and why?","['A.No; the Delta Lake DELETE command only provides ACID guarantees when combined with the MERGE INTO command.', 'B.No; files containing deleted records may still be accessible with time travel until a VACUUM command is used to remove invalidated data files.', 'C.No; the change data feed only tracks inserts and updates, not deleted records.', 'D.Yes; Delta Lake ACID guarantees provide assurance that the DELETE command succeeded fully and permanently purged these records.']",['B.No; files containing deleted records may still be accessible with time travel until a VACUUM command is used to remove invalidated data files.'],['https://img.examtopics.com/certified-data-engineer-professional/image72.png'],
166,examtopics databricks data engineer professional certification question 166,https://www.examtopics.com/discussions/databricks/view/141559-exam-certified-data-engineer-professional-topic-1-question/,"An external object storage container has been mounted to the location /mnt/finance_eda_bucket.The following logic was executed to create a database for the finance team:After the database was successfully created and permissions configured, a member of the finance team runs the following code:If all users on the finance team are members of the finance group, which statement describes how the tx_sales table will be created?","['A.A logical table will persist the query plan to the Hive Metastore in the Databricks control plane.', 'B.An external table will be created in the storage container mounted to /mnt/finance_eda_bucket.', 'C.A managed table will be created in the DBFS root storage container.', 'D.An managed table will be created in the storage container mounted to /mnt/finance_eda_bucket.']",['D.An managed table will be created in the storage container mounted to /mnt/finance_eda_bucket.'],"['https://img.examtopics.com/certified-data-engineer-professional/image73.png', 'https://img.examtopics.com/certified-data-engineer-professional/image74.png']",
167,examtopics databricks data engineer professional certification question 167,https://www.examtopics.com/discussions/databricks/view/153066-exam-certified-data-engineer-associate-topic-1-question-167/,"A data engineer is designing a data pipeline. The source system generates files in a shared directory that is also used by other processes. As a result, the files should be kept as is and will accumulate in the directory. The data engineer needs to identify which files are new since the previous run in the pipeline, and set up the pipeline to only ingest those new files with each run.Which of the following tools can the data engineer use to solve this problem?","['A.Unity Catalog', 'B.Delta Lake', 'C.Databricks SQL', 'D.Auto Loader']",['D.Auto Loader'],[],
168,examtopics databricks data engineer professional certification question 168,https://www.examtopics.com/discussions/databricks/view/141707-exam-certified-data-engineer-professional-topic-1-question/,What is the retention of job run history?,"['A.It is retained until you export or delete job run logs', 'B.It is retained for 30 days, during which time you can deliver job run logs to DBFS or S3', 'C.It is retained for 60 days, during which you can export notebook run results to HTML', 'D.It is retained for 60 days, after which logs are archived']","['C.It is retained for 60 days, during which you can export notebook run results to HTML']",[],
169,examtopics databricks data engineer professional certification question 169,https://www.examtopics.com/discussions/databricks/view/141706-exam-certified-data-engineer-professional-topic-1-question/,"A data engineer, User A, has promoted a new pipeline to production by using the REST API to programmatically create several jobs. A DevOps engineer, User B, has configured an external orchestration tool to trigger job runs through the REST API. Both users authorized the REST API calls using their personal access tokens.Which statement describes the contents of the workspace audit logs concerning these events?","['A.Because the REST API was used for job creation and triggering runs, a Service Principal will be automatically used to identify these events.', 'B.Because User A created the jobs, their identity will be associated with both the job creation events and the job run events.', 'C.Because these events are managed separately, User A will have their identity associated with the job creation events and User B will have their identity associated with the job run events.', 'D.Because the REST API was used for job creation and triggering runs, user identity will not be captured in the audit logs.']","['C.Because these events are managed separately, User A will have their identity associated with the job creation events and User B will have their identity associated with the job run events.']",[],
170,examtopics databricks data engineer professional certification question 170,https://www.examtopics.com/discussions/databricks/view/141705-exam-certified-data-engineer-professional-topic-1-question/,"A distributed team of data analysts share computing resources on an interactive cluster with autoscaling configured. In order to better manage costs and query throughput, the workspace administrator is hoping to evaluate whether cluster upscaling is caused by many concurrent users or resource-intensive queries.In which location can one review the timeline for cluster resizing events?","['A.Workspace audit logs', ""B.Driver's log file"", 'C.Ganglia', 'D.Cluster Event Log']",['D.Cluster Event Log'],[],
171,examtopics databricks data engineer professional certification question 171,https://www.examtopics.com/discussions/databricks/view/141704-exam-certified-data-engineer-professional-topic-1-question/,"When evaluating the Ganglia Metrics for a given cluster with 3 executor nodes, which indicator would signal proper utilization of the VM's resources?","['A.The five Minute Load Average remains consistent/flat', 'B.CPU Utilization is around 75%', 'C.Network I/O never spikes', 'D.Total Disk Space remains constant']",['B.CPU Utilization is around 75%'],[],
172,examtopics databricks data engineer professional certification question 172,https://www.examtopics.com/discussions/databricks/view/141561-exam-certified-data-engineer-professional-topic-1-question/,The data engineer is using Spark's MEMORY_ONLY storage level.Which indicators should the data engineer look for in the Spark UI's Storage tab to signal that a cached table is not performing optimally?,"['A.On Heap Memory Usage is within 75% of Off Heap Memory Usage', 'B.The RDD Block Name includes the “*” annotation signaling a failure to cache', 'C.Size on Disk is > 0', 'D.The number of Cached Partitions > the number of Spark Partitions']",['C.Size on Disk is > 0'],[],
173,examtopics databricks data engineer professional certification question 173,https://www.examtopics.com/discussions/databricks/view/141703-exam-certified-data-engineer-professional-topic-1-question/,Review the following error traceback:Which statement describes the error being raised?,"['A.There is a syntax error because the heartrate column is not correctly identified as a column.', 'B.There is no column in the table named heartrateheartrateheartrate', 'C.There is a type error because a column object cannot be multiplied.', 'D.There is a type error because a DataFrame object cannot be multiplied.']",['B.There is no column in the table named heartrateheartrateheartrate'],['https://img.examtopics.com/certified-data-engineer-professional/image75.png'],
174,examtopics databricks data engineer professional certification question 174,https://www.examtopics.com/discussions/databricks/view/141702-exam-certified-data-engineer-professional-topic-1-question/,What is a method of installing a Python package scoped at the notebook level to all nodes in the currently active cluster?,"['A.Run source env/bin/activate in a notebook setup script', 'B.Install libraries from PyPI using the cluster UI', 'C.Use %pip install in a notebook cell', 'D.Use %sh pip install in a notebook cell']",['C.Use %pip install in a notebook cell'],[],
175,examtopics databricks data engineer professional certification question 175,https://www.examtopics.com/discussions/databricks/view/312870-exam-certified-data-engineer-associate-topic-1-question-175/,A Databricks single-task workflow fails at the last task due to an error in a notebook. The data engineer fixes the mistake in the notebook.What should the data engineer do to rerun the workflow?,"['A.Repair the task', 'B.Rerun the pipeline', 'C.Restart the cluster', 'D.Switch the cluster']",['A.Repair the task'],[],
176,examtopics databricks data engineer professional certification question 176,https://www.examtopics.com/discussions/databricks/view/141700-exam-certified-data-engineer-professional-topic-1-question/,"Incorporating unit tests into a PySpark application requires upfront attention to the design of your jobs, or a potentially significant refactoring of existing code.Which benefit offsets this additional effort?","['A.Improves the quality of your data', 'B.Validates a complete use case of your application', 'C.Troubleshooting is easier since all steps are isolated and tested individually', 'D.Ensures that all steps interact correctly to achieve the desired end result']",['C.Troubleshooting is easier since all steps are isolated and tested individually'],[],
177,examtopics databricks data engineer professional certification question 177,https://www.examtopics.com/discussions/databricks/view/141699-exam-certified-data-engineer-professional-topic-1-question/,What describes integration testing?,"['A.It validates an application use case.', 'B.It validates behavior of individual elements of an application,', 'C.It requires an automated testing framework.', 'D.It validates interactions between subsystems of your application.']",['D.It validates interactions between subsystems of your application.'],[],
178,examtopics databricks data engineer professional certification question 178,https://www.examtopics.com/discussions/databricks/view/141698-exam-certified-data-engineer-professional-topic-1-question/,The Databricks CLI is used to trigger a run of an existing job by passing the job_id parameter. The response that the job run request has been submitted successfully includes a field run_id.Which statement describes what the number alongside this field represents?,"['A.The job_id and number of times the job has been run are concatenated and returned.', 'B.The globally unique ID of the newly triggered run.', 'C.The number of times the job definition has been run in this workspace.', 'D.The job_id is returned in this field.']",['B.The globally unique ID of the newly triggered run.'],[],
179,examtopics databricks data engineer professional certification question 179,https://www.examtopics.com/discussions/databricks/view/141697-exam-certified-data-engineer-professional-topic-1-question/,"A Databricks job has been configured with three tasks, each of which is a Databricks notebook. Task A does not depend on other tasks. Tasks B and C run in parallel, with each having a serial dependency on task A.What will be the resulting state if tasks A and B complete successfully but task C fails during a scheduled run?","['A.All logic expressed in the notebook associated with tasks A and B will have been successfully completed; some operations in task C may have completed successfully.', 'B.Unless all tasks complete successfully, no changes will be committed to the Lakehouse; because task C failed, all commits will be rolled back automatically.', 'C.Because all tasks are managed as a dependency graph, no changes will be committed to the Lakehouse until all tasks have successfully been completed.', 'D.All logic expressed in the notebook associated with tasks A and B will have been successfully completed; any changes made in task C will be rolled back due to task failure.']",['A.All logic expressed in the notebook associated with tasks A and B will have been successfully completed; some operations in task C may have completed successfully.'],[],
180,examtopics databricks data engineer professional certification question 180,https://www.examtopics.com/discussions/databricks/view/154490-exam-certified-data-engineer-professional-topic-1-question/,"When scheduling Structured Streaming jobs for production, which configuration automatically recovers from query failures and keeps costs low?","['A.Cluster: New Job Cluster;Retries: Unlimited;Maximum Concurrent Runs: 1', 'B.Cluster: New Job Cluster;Retries: Unlimited;Maximum Concurrent Runs: Unlimited', 'C.Cluster: Existing All-Purpose Cluster;Retries: Unlimited;Maximum Concurrent Runs: 1', 'D.Cluster: New Job Cluster;Retries: None;Maximum Concurrent Runs: 1']",['A.Cluster: New Job Cluster;Retries: Unlimited;Maximum Concurrent Runs: 1'],[],
181,examtopics databricks data engineer professional certification question 181,https://www.examtopics.com/discussions/databricks/view/154677-exam-certified-data-engineer-professional-topic-1-question/,"A Delta Lake table was created with the below query:Realizing that the original query had a typographical error, the below code was executed:ALTER TABLE prod.sales_by_stor RENAME TO prod.sales_by_storeWhich result will occur after running the second command?","['A.The table reference in the metastore is updated.', 'B.All related files and metadata are dropped and recreated in a single ACID transaction.', 'C.The table name change is recorded in the Delta transaction log.', 'D.A new Delta transaction log is created for the renamed table.']",['A.The table reference in the metastore is updated.'],['https://img.examtopics.com/certified-data-engineer-professional/image76.png'],
182,examtopics databricks data engineer professional certification question 182,https://www.examtopics.com/discussions/databricks/view/152629-exam-certified-data-engineer-professional-topic-1-question/,"The data engineering team has configured a Databricks SQL query and alert to monitor the values in a Delta Lake table. The recent_sensor_recordings table contains an identifying sensor_id alongside the timestamp and temperature for the most recent 5 minutes of recordings.The below query is used to create the alert:The query is set to refresh each minute and always completes in less than 10 seconds. The alert is set to trigger when mean (temperature) > 120. Notifications are triggered to be sent at most every 1 minute.If this alert raises notifications for 3 consecutive minutes and then stops, which statement must be true?","['A.The total average temperature across all sensors exceeded 120 on three consecutive executions of the query', 'B.The average temperature recordings for at least one sensor exceeded 120 on three consecutive executions of the query', 'C.The source query failed to update properly for three consecutive minutes and then restarted', 'D.The maximum temperature recording for at least one sensor exceeded 120 on three consecutive executions of the query']",['B.The average temperature recordings for at least one sensor exceeded 120 on three consecutive executions of the query'],['https://img.examtopics.com/certified-data-engineer-professional/image77.png'],
183,examtopics databricks data engineer professional certification question 183,https://www.examtopics.com/discussions/databricks/view/303224-exam-certified-data-engineer-professional-topic-1-question/,"A junior developer complains that the code in their notebook isn't producing the correct results in the development environment. A shared screenshot reveals that while they're using a notebook versioned with Databricks Repos, they're using a personal branch that contains old logic. The desired branch named dev-2.3.9 is not available from the branch selection dropdown.Which approach will allow this developer to review the current logic for this notebook?","['A.Use Repos to make a pull request use the Databricks REST API to update the current branch to dev-2.3.9', 'B.Use Repos to pull changes from the remote Git repository and select the dev-2.3.9 branch.', 'C.Use Repos to checkout the dev-2.3.9 branch and auto-resolve conflicts with the current branch', 'D.Use Repos to merge the current branch and the dev-2.3.9 branch, then make a pull request to sync with the remote repository']",['B.Use Repos to pull changes from the remote Git repository and select the dev-2.3.9 branch.'],[],
184,examtopics databricks data engineer professional certification question 184,https://www.examtopics.com/discussions/databricks/view/303225-exam-certified-data-engineer-professional-topic-1-question/,Two of the most common data locations on Databricks are the DBFS root storage and external object storage mounted with dbutils.fs.mount().Which of the following statements is correct?,"['A.DBFS is a file system protocol that allows users to interact with files stored in object storage using syntax and guarantees similar to Unix file systems.', 'B.By default, both the DBFS root and mounted data sources are only accessible to workspace administrators.', 'C.The DBFS root is the most secure location to store data, because mounted storage volumes must have full public read and write permissions.', 'D.The DBFS root stores files in ephemeral block volumes attached to the driver, while mounted directories will always persist saved data to external storage between sessions.']",['A.DBFS is a file system protocol that allows users to interact with files stored in object storage using syntax and guarantees similar to Unix file systems.'],[],
185,examtopics databricks data engineer professional certification question 185,https://www.examtopics.com/discussions/databricks/view/154493-exam-certified-data-engineer-professional-topic-1-question/,"An upstream system has been configured to pass the date for a given batch of data to the Databricks Jobs API as a parameter. The notebook to be scheduled will use this parameter to load data with the following code:df = spark.read.format(""parquet"").load(f""/mnt/source/(date)"")Which code block should be used to create the date Python variable used in the above code block?","['A.date = spark.conf.get(""date"")', 'B.import sysdate = sys.argv[1]', 'C.date = dbutils.notebooks.getParam(""date"")', 'D.dbutils.widgets.text(""date"", ""null"")date = dbutils.widgets.get(""date"")']","['D.dbutils.widgets.text(""date"", ""null"")date = dbutils.widgets.get(""date"")']",[],
186,examtopics databricks data engineer professional certification question 186,https://www.examtopics.com/discussions/databricks/view/152574-exam-certified-data-engineer-professional-topic-1-question/,"The Databricks workspace administrator has configured interactive clusters for each of the data engineering groups. To control costs, clusters are set to terminate after 30 minutes of inactivity. Each user should be able to execute workloads against their assigned clusters at any time of the day.Assuming users have been added to a workspace but not granted any permissions, which of the following describes the minimal permissions a user would need to start and attach to an already configured cluster.","['A.""Can Manage"" privileges on the required cluster', 'B.Cluster creation allowed, ""Can Restart"" privileges on the required cluster', 'C.Cluster creation allowed, ""Can Attach To"" privileges on the required cluster', 'D.""Can Restart"" privileges on the required cluster']","['D.""Can Restart"" privileges on the required cluster']",[],
187,examtopics databricks data engineer professional certification question 187,https://www.examtopics.com/discussions/databricks/view/152575-exam-certified-data-engineer-professional-topic-1-question/,"The data science team has created and logged a production model using MLflow. The following code correctly imports and applies the production model to output the predictions as a new DataFrame named preds with the schema ""customer_id LONG, predictions DOUBLE, date DATE"".The data science team would like predictions saved to a Delta Lake table with the ability to compare all predictions across time. Churn predictions will be made at most once per day.Which code block accomplishes this task while minimizing potential compute costs?","['A.preds.write.mode(""append"").saveAsTable(""churn_preds"")', 'B.preds.write.format(""delta"").save(""/preds/churn_preds"")', 'C.', 'D.']","['A.preds.write.mode(""append"").saveAsTable(""churn_preds"")']",['https://img.examtopics.com/certified-data-engineer-professional/image78.png'],
188,examtopics databricks data engineer professional certification question 188,https://www.examtopics.com/discussions/databricks/view/303226-exam-certified-data-engineer-professional-topic-1-question/,"The following code has been migrated to a Databricks notebook from a legacy workload:The code executes successfully and provides the logically correct results, however, it takes over 20 minutes to extract and load around 1 GB of data.Which statement is a possible explanation for this behavior?","['A.%sh triggers a cluster restart to collect and install Git. Most of the latency is related to cluster startup time.', 'B.Instead of cloning, the code should use %sh pip install so that the Python code can get executed in parallel across all nodes in a cluster.', 'C.%sh does not distribute file moving operations; the final line of code should be updated to use %fs instead.', 'D.%sh executes shell code on the driver node. The code does not take advantage of the worker nodes or Databricks optimized Spark.']",['D.%sh executes shell code on the driver node. The code does not take advantage of the worker nodes or Databricks optimized Spark.'],['https://img.examtopics.com/certified-data-engineer-professional/image81.png'],
189,examtopics databricks data engineer professional certification question 189,https://www.examtopics.com/discussions/databricks/view/152577-exam-certified-data-engineer-professional-topic-1-question/,"A Delta table of weather records is partitioned by date and has the below schema:date DATE, device_id INT, temp FLOAT, latitude FLOAT, longitude FLOATTo find all the records from within the Arctic Circle, you execute a query with the below filter:latitude > 66.3Which statement describes how the Delta engine identifies which files to load?","['A.All records are cached to an operational database and then the filter is applied', 'B.The Parquet file footers are scanned for min and max statistics for the latitude column', 'C.The Hive metastore is scanned for min and max statistics for the latitude column', 'D.The Delta log is scanned for min and max statistics for the latitude column']",['D.The Delta log is scanned for min and max statistics for the latitude column'],[],
190,examtopics databricks data engineer professional certification question 190,https://www.examtopics.com/discussions/databricks/view/154494-exam-certified-data-engineer-professional-topic-1-question/,"In order to prevent accidental commits to production data, a senior data engineer has instituted a policy that all development work will reference clones of Delta Lake tables. After testing both DEEP and SHALLOW CLONE, development tables are created using SHALLOW CLONE.A few weeks after initial table creation, the cloned versions of several tables implemented as Type 1 Slowly Changing Dimension (SCD) stop working. The transaction logs for the source tables show that VACUUM was run the day before.Which statement describes why the cloned tables are no longer working?","['A.Because Type 1 changes overwrite existing records, Delta Lake cannot guarantee data consistency for cloned tables.', 'B.Running VACUUM automatically invalidates any shallow clones of a table; DEEP CLONE should always be used when a cloned table will be repeatedly queried.', 'C.The data files compacted by VACUUM are not tracked by the cloned metadata; running REFRESH on the cloned table will pull in recent changes.', 'D.The metadata created by the CLONE operation is referencing data files that were purged as invalid by the VACUUM command.']",['D.The metadata created by the CLONE operation is referencing data files that were purged as invalid by the VACUUM command.'],[],
191,examtopics databricks data engineer professional certification question 191,https://www.examtopics.com/discussions/databricks/view/152579-exam-certified-data-engineer-professional-topic-1-question/,"A junior data engineer has configured a workload that posts the following JSON to the Databricks REST API endpoint 2.0/jobs/create.Assuming that all configurations and referenced resources are available, which statement describes the result of executing this workload three times?","['A.The logic defined in the referenced notebook will be executed three times on the referenced existing all purpose cluster.', 'B.The logic defined in the referenced notebook will be executed three times on new clusters with the configurations of the provided cluster ID.', 'C.Three new jobs named ""Ingest new data"" will be defined in the workspace, but no jobs will be executed.', 'D.One new job named ""Ingest new data"" will be defined in the workspace, but it will not be executed.']","['C.Three new jobs named ""Ingest new data"" will be defined in the workspace, but no jobs will be executed.']",['https://img.examtopics.com/certified-data-engineer-professional/image82.png'],
192,examtopics databricks data engineer professional certification question 192,https://www.examtopics.com/discussions/databricks/view/303227-exam-certified-data-engineer-professional-topic-1-question/,"A Delta Lake table in the Lakehouse named customer_churn_params is used in churn prediction by the machine learning team. The table contains information about customers derived from a number of upstream sources. Currently, the data engineering team populates this table nightly by overwriting the table with the current valid values derived from upstream data sources.Immediately after each update succeeds, the data engineering team would like to determine the difference between the new version and the previous version of the table.Given the current implementation, which method can be used?","['A.Execute a query to calculate the difference between the new version and the previous version using Delta Lake’s built-in versioning and lime travel functionality.', 'B.Parse the Delta Lake transaction log to identify all newly written data files.', 'C.Parse the Spark event logs to identify those rows that were updated, inserted, or deleted.', 'D.Execute DESCRIBE HISTORY customer_churn_params to obtain the full operation metrics for the update, including a log of all records that have been added or modified.']",['A.Execute a query to calculate the difference between the new version and the previous version using Delta Lake’s built-in versioning and lime travel functionality.'],[],
193,examtopics databricks data engineer professional certification question 193,https://www.examtopics.com/discussions/databricks/view/152580-exam-certified-data-engineer-professional-topic-1-question/,A view is registered with the following code:Both users and orders are Delta Lake tables.Which statement describes the results of querying recent_orders?,"['A.The versions of each source table will be stored in the table transaction log; query results will be saved to DBFS with each query.', 'B.All logic will execute when the table is defined and store the result of joining tables to the DBFS; this stored data will be returned when the table is queried.', 'C.All logic will execute at query time and return the result of joining the valid versions of the source tables at the time the query finishes.', 'D.All logic will execute at query time and return the result of joining the valid versions of the source tables at the time the query began.']",['D.All logic will execute at query time and return the result of joining the valid versions of the source tables at the time the query began.'],['https://img.examtopics.com/certified-data-engineer-professional/image83.png'],
194,examtopics databricks data engineer professional certification question 194,https://www.examtopics.com/discussions/databricks/view/152581-exam-certified-data-engineer-professional-topic-1-question/,A data engineer is performing a join operation to combine values from a static userLookup table with a streaming DataFrame streamingDF.Which code block attempts to perform an invalid stream-static join?,"['A.userLookup.join(streamingDF, [""user_id""], how=""right"")', 'B.streamingDF.join(userLookup, [""user_id""], how=""inner"")', 'C.userLookup.join(streamingDF, [""user_id""), how=""inner"")', 'D.userLookup.join(streamingDF, [""user_id""], how=""left"")']","['D.userLookup.join(streamingDF, [""user_id""], how=""left"")']",[],
195,examtopics databricks data engineer professional certification question 195,https://www.examtopics.com/discussions/databricks/view/303297-exam-certified-data-engineer-professional-topic-1-question/,"A junior data engineer has been asked to develop a streaming data pipeline with a grouped aggregation using DataFrame df. The pipeline needs to calculate the average humidity and average temperature for each non-overlapping five-minute interval. Incremental state information should be maintained for 10 minutes for late-arriving data.Streaming DataFrame df has the following schema:""device_id INT, event_time TIMESTAMP, temp FLOAT, humidity FLOAT""Code block:Choose the response that correctly fills in the blank within the code block to complete this task.","['A.withWatermark(""event_time"", ""10 minutes"")', 'B.awaitArrival(""event_time"", ""10 minutes"")', 'C.await(""event_time + ‘10 minutes\'"")', 'D.slidingWindow(""event_time"", ""10 minutes"")']","['A.withWatermark(""event_time"", ""10 minutes"")']",['https://img.examtopics.com/certified-data-engineer-professional/image84.png'],
196,examtopics databricks data engineer professional certification question 196,https://www.examtopics.com/discussions/databricks/view/152615-exam-certified-data-engineer-professional-topic-1-question/,"A data architect has designed a system in which two Structured Streaming jobs will concurrently write to a single bronze Delta table. Each job is subscribing to a different topic from an Apache Kafka source, but they will write data with the same schema. To keep the directory structure simple, a data engineer has decided to nest a checkpoint directory to be shared by both streams.The proposed directory structure is displayed below:Which statement describes whether this checkpoint directory structure is valid for the given scenario and why?","['A.No; Delta Lake manages streaming checkpoints in the transaction log.', 'B.Yes; both of the streams can share a single checkpoint directory.', 'C.No; only one stream can write to a Delta Lake table.', 'D.No; each of the streams needs to have its own checkpoint directory.']",['D.No; each of the streams needs to have its own checkpoint directory.'],['https://img.examtopics.com/certified-data-engineer-professional/image85.png'],
197,examtopics databricks data engineer professional certification question 197,https://www.examtopics.com/discussions/databricks/view/303298-exam-certified-data-engineer-professional-topic-1-question/,"A Structured Streaming job deployed to production has been experiencing delays during peak hours of the day. At present, during normal execution, each microbatch of data is processed in less than 3 seconds. During peak hours of the day, execution time for each microbatch becomes very inconsistent, sometimes exceeding 30 seconds. The streaming write is currently configured with a trigger interval of 10 seconds.Holding all other variables constant and assuming records need to be processed in less than 10 seconds, which adjustment will meet the requirement?","['A.Decrease the trigger interval to 5 seconds; triggering batches more frequently allows idle executors to begin processing the next batch while longer running tasks from previous batches finish.', 'B.Decrease the trigger interval to 5 seconds; triggering batches more frequently may prevent records from backing up and large batches from causing spill.', 'C.The trigger interval cannot be modified without modifying the checkpoint directory; to maintain the current stream state, increase the number of shuffle partitions to maximize parallelism.', 'D.Use the trigger once option and configure a Databricks job to execute the query every 10 seconds; this ensures all backlogged records are processed with each batch.']",['B.Decrease the trigger interval to 5 seconds; triggering batches more frequently may prevent records from backing up and large batches from causing spill.'],[],
198,examtopics databricks data engineer professional certification question 198,https://www.examtopics.com/discussions/databricks/view/152616-exam-certified-data-engineer-professional-topic-1-question/,Which statement describes the default execution mode for Databricks Auto Loader?,"['A.Cloud vendor-specific queue storage and notification services are configured to track newly arriving files; new files are incrementally and idempotently loaded into the target Delta Lake table.', 'B.New files are identified by listing the input directory; the target table is materialized by directly querying all valid files in the source directory.', 'C.Webhooks trigger a Databricks job to run anytime new data arrives in a source directory; new data are automatically merged into target tables using rules inferred from the data.', 'D.New files are identified by listing the input directory; new files are incrementally and idempotently loaded into the target Delta Lake table.']",['D.New files are identified by listing the input directory; new files are incrementally and idempotently loaded into the target Delta Lake table.'],[],
199,examtopics databricks data engineer professional certification question 199,https://www.examtopics.com/discussions/databricks/view/303301-exam-certified-data-engineer-professional-topic-1-question/,Which statement describes the correct use of pyspark.sql.functions.broadcast?,"['A.It marks a column as having low enough cardinality to properly map distinct values to available partitions, allowing a broadcast join.', 'B.It marks a column as small enough to store in memory on all executors, allowing a broadcast join.', 'C.It caches a copy of the indicated table on all nodes in the cluster for use in all future queries during the cluster lifetime.', 'D.It marks a DataFrame as small enough to store in memory on all executors, allowing a broadcast join.']","['D.It marks a DataFrame as small enough to store in memory on all executors, allowing a broadcast join.']",[],
200,examtopics databricks data engineer professional certification question 200,https://www.examtopics.com/discussions/databricks/view/303303-exam-certified-data-engineer-professional-topic-1-question/,"Spill occurs as a result of executing various wide transformations. However, diagnosing spill requires one to proactively look for key indicators.Where in the Spark UI are two of the primary indicators that a partition is spilling to disk?","['A.Stage’s detail screen and Query’s detail screen', 'B.Stage’s detail screen and Executor’s log files', 'C.Driver’s and Executor’s log files', 'D.Executor’s detail screen and Executor’s log files']",['B.Stage’s detail screen and Executor’s log files'],[],
201,examtopics databricks data engineer professional certification question 201,https://www.examtopics.com/discussions/databricks/view/154495-exam-certified-data-engineer-professional-topic-1-question/,"An upstream source writes Parquet data as hourly batches to directories named with the current date. A nightly batch job runs the following code to ingest all data from the previous day as indicated by the date variable:Assume that the fields customer_id and order_id serve as a composite key to uniquely identify each order.If the upstream system is known to occasionally produce duplicate entries for a single order hours apart, which statement is correct?","['A.Each write to the orders table will only contain unique records, and only those records without duplicates in the target table will be written.', 'B.Each write to the orders table will only contain unique records, but newly written records may have duplicates already present in the target table.', 'C.Each write to the orders table will only contain unique records; if existing records with the same key are present in the target table, these records will be overwritten.', 'D.Each write to the orders table will run deduplication over the union of new and existing records, ensuring no duplicate records are present.']","['B.Each write to the orders table will only contain unique records, but newly written records may have duplicates already present in the target table.']",['https://img.examtopics.com/certified-data-engineer-professional/image86.png'],
202,examtopics databricks data engineer professional certification question 202,https://www.examtopics.com/discussions/databricks/view/169466-exam-certified-data-engineer-professional-topic-1-question/,"A junior data engineer on your team has implemented the following code block.The view new_events contains a batch of records with the same schema as the events Delta table. The event_id field serves as a unique key for this table.When this query is executed, what will happen with new records that have the same event_id as an existing record?","['A.They are merged.', 'B.They are ignored.', 'C.They are updated.', 'D.They are inserted.']",['B.They are ignored.'],['https://img.examtopics.com/certified-data-engineer-professional/image87.png'],
203,examtopics databricks data engineer professional certification question 203,https://www.examtopics.com/discussions/databricks/view/303304-exam-certified-data-engineer-professional-topic-1-question/,"A new data engineer notices that a critical field was omitted from an application that writes its Kafka source to Delta Lake. This happened even though the critical field was in the Kafka source. That field was further missing from data written to dependent, long-term storage. The retention threshold on the Kafka service is seven days. The pipeline has been in production for three months.Which describes how Delta Lake can help to avoid data loss of this nature in the future?","['A.The Delta log and Structured Streaming checkpoints record the full history of the Kafka producer.', 'B.Delta Lake schema evolution can retroactively calculate the correct value for newly added fields, as long as the data was in the original source.', 'C.Delta Lake automatically checks that all fields present in the source data are included in the ingestion layer.', 'D.Ingesting all raw data and metadata from Kafka to a bronze Delta table creates a permanent, replayable history of the data state.']","['D.Ingesting all raw data and metadata from Kafka to a bronze Delta table creates a permanent, replayable history of the data state.']",[],
204,examtopics databricks data engineer professional certification question 204,https://www.examtopics.com/discussions/databricks/view/154496-exam-certified-data-engineer-professional-topic-1-question/,"The data engineering team maintains the following code:Assuming that this code produces logically correct results and the data in the source table has been de-duplicated and validated, which statement describes what will occur when this code is executed?","['A.The silver_customer_sales table will be overwritten by aggregated values calculated from all records in the gold_customer_lifetime_sales_summary table as a batch job.', 'B.A batch job will update the gold_customer_lifetime_sales_summary table, replacing only those rows that have different values than the current version of the table, using customer_id as the primary key.', 'C.The gold_customer_lifetime_sales_summary table will be overwritten by aggregated values calculated from all records in the silver_customer_sales table as a batch job.', 'D.An incremental job will detect if new rows have been written to the silver_customer_sales table; if new rows are detected, all aggregates will be recalculated and used to overwrite the gold_customer_lifetime_sales_summary table.']",['C.The gold_customer_lifetime_sales_summary table will be overwritten by aggregated values calculated from all records in the silver_customer_sales table as a batch job.'],['https://img.examtopics.com/certified-data-engineer-professional/image88.png'],
205,examtopics databricks data engineer professional certification question 205,https://www.examtopics.com/discussions/databricks/view/303311-exam-certified-data-engineer-professional-topic-1-question/,"The data engineering team is migrating an enterprise system with thousands of tables and views into the Lakehouse. They plan to implement the target architecture using a series of bronze, silver, and gold tables. Bronze tables will almost exclusively be used by production data engineering workloads, while silver tables will be used to support both data engineering and machine learning workloads. Gold tables will largely serve business intelligence and reporting purposes. While personal identifying information (PII) exists in all tiers of data, pseudonymization and anonymization rules are in place for all data at the silver and gold levels.The organization is interested in reducing security concerns while maximizing the ability to collaborate across diverse teams.Which statement exemplifies best practices for implementing this system?","['A.Isolating tables in separate databases based on data quality tiers allows for easy permissions management through database ACLs and allows physical separation of default storage locations for managed tables.', 'B.Because databases on Databricks are merely a logical construct, choices around database organization do not impact security or discoverability in the Lakehouse.', 'C.Storing all production tables in a single database provides a unified view of all data assets available throughout the Lakehouse, simplifying discoverability by granting all users view privileges on this database.', 'D.Working in the default Databricks database provides the greatest security when working with managed tables, as these will be created in the DBFS root.']",['A.Isolating tables in separate databases based on data quality tiers allows for easy permissions management through database ACLs and allows physical separation of default storage locations for managed tables.'],[],
206,examtopics databricks data engineer professional certification question 206,https://www.examtopics.com/discussions/databricks/view/303314-exam-certified-data-engineer-professional-topic-1-question/,"The data architect has mandated that all tables in the Lakehouse should be configured as external (also known as ""unmanaged"") Delta Lake tables.Which approach will ensure that this requirement is met?","['A.When a database is being created, make sure that the LOCATION keyword is used.', 'B.When the workspace is being configured, make sure that external cloud object storage has been mounted.', 'C.When data is saved to a table, make sure that a full file path is specified alongside the USING DELTA clause.', 'D.When tables are created, make sure that the UNMANAGED keyword is used in the CREATE TABLE statement.']","['C.When data is saved to a table, make sure that a full file path is specified alongside the USING DELTA clause.']",[],
207,examtopics databricks data engineer professional certification question 207,https://www.examtopics.com/discussions/databricks/view/303315-exam-certified-data-engineer-professional-topic-1-question/,"To reduce storage and compute costs, the data engineering team has been tasked with curating a series of aggregate tables leveraged by business intelligence dashboards, customer-facing applications, production machine learning models, and ad hoc analytical queries.The data engineering team has been made aware of new requirements from a customer-facing application, which is the only downstream workload they manage entirely. As a result, an aggregate table used by numerous teams across the organization will need to have a number of fields renamed, and additional fields will also be added.Which of the solutions addresses the situation while minimally interrupting other teams in the organization without increasing the number of tables that need to be managed?","['A.Send all users notice that the schema for the table will be changing; include in the communication the logic necessary to revert the new table schema to match historic queries.', 'B.Configure a new table with all the requisite fields and new names and use this as the source for the customer-facing application; create a view that maintains the original data schema and table name by aliasing select fields from the new table.', ""C.Create a new table with the required schema and new fields and use Delta Lake's deep clone functionality to sync up changes committed to one table to the corresponding table."", 'D.Replace the current table definition with a logical view defined with the query logic currently writing the aggregate table; create a new table to power the customer-facing application.']",['B.Configure a new table with all the requisite fields and new names and use this as the source for the customer-facing application; create a view that maintains the original data schema and table name by aliasing select fields from the new table.'],[],
208,examtopics databricks data engineer professional certification question 208,https://www.examtopics.com/discussions/databricks/view/303316-exam-certified-data-engineer-professional-topic-1-question/,"A Delta Lake table representing metadata about content posts from users has the following schema:user_id LONG, post_text STRING, post_id STRING, longitude FLOAT, latitude FLOAT, post_time TIMESTAMP, date DATEBased on the above schema, which column is a good candidate for partitioning the Delta Table?","['A.post_time', 'B.date', 'C.post_id', 'D.user_id']",['B.date'],[],
209,examtopics databricks data engineer professional certification question 209,https://www.examtopics.com/discussions/databricks/view/303317-exam-certified-data-engineer-professional-topic-1-question/,"The downstream consumers of a Delta Lake table have been complaining about data quality issues impacting performance in their applications. Specifically, they have complained that invalid latitude and longitude values in the activity_details table have been breaking their ability to use other geolocation processes.A junior engineer has written the following code to add CHECK constraints to the Delta Lake table:A senior engineer has confirmed the above logic is correct and the valid ranges for latitude and longitude are provided, but the code fails when executed.Which statement explains the cause of this failure?","['A.The current table schema does not contain the field valid_coordinates; schema evolution will need to be enabled before altering the table to add a constraint.', 'B.The activity_details table already exists; CHECK constraints can only be added during initial table creation.', 'C.The activity_details table already contains records that violate the constraints; all existing data must pass CHECK constraints in order to add them to an existing table.', 'D.The activity_details table already contains records; CHECK constraints can only be added prior to inserting values into a table.']",['C.The activity_details table already contains records that violate the constraints; all existing data must pass CHECK constraints in order to add them to an existing table.'],['https://img.examtopics.com/certified-data-engineer-professional/image89.png'],
210,examtopics databricks data engineer professional certification question 210,https://www.examtopics.com/discussions/databricks/view/303318-exam-certified-data-engineer-professional-topic-1-question/,What is true for Delta Lake?,"['A.Views in the Lakehouse maintain a valid cache of the most recent versions of source tables at all times.', 'B.Primary and foreign key constraints can be leveraged to ensure duplicate values are never entered into a dimension table.', 'C.Delta Lake automatically collects statistics on the first 32 columns of each table which are leveraged in data skipping based on query filters.', 'D.Z-order can only be applied to numeric values stored in Delta Lake tables.']",['C.Delta Lake automatically collects statistics on the first 32 columns of each table which are leveraged in data skipping based on query filters.'],[],
211,examtopics databricks data engineer professional certification question 211,https://www.examtopics.com/discussions/databricks/view/303319-exam-certified-data-engineer-professional-topic-1-question/,The view updates represents an incremental batch of all newly ingested data to be inserted or updated in the customers table.The following logic is used to process these records.Which statement describes this implementation?,"['A.The customers table is implemented as a Type 2 table; old values are overwritten and new customers are appended.', 'B.The customers table is implemented as a Type 2 table; old values are maintained but marked as no longer current and new values are inserted.', 'C.The customers table is implemented as a Type 0 table; all writes are append only with no changes to existing values.', 'D.The customers table is implemented as a Type 1 table; old values are overwritten by new values and no history is maintained.']",['B.The customers table is implemented as a Type 2 table; old values are maintained but marked as no longer current and new values are inserted.'],['https://img.examtopics.com/certified-data-engineer-professional/image90.png'],
212,examtopics databricks data engineer professional certification question 212,https://www.examtopics.com/discussions/databricks/view/152709-exam-certified-data-engineer-professional-topic-1-question/,A team of data engineers are adding tables to a DLT pipeline that contain repetitive expectations for many of the same data quality checks. One member of the team suggests reusing these data quality rules across all tables defined for this pipeline.What approach would allow them to do this?,"['A.Add data quality constraints to tables in this pipeline using an external job with access to pipeline configuration files.', 'B.Use global Python variables to make expectations visible across DLT notebooks included in the same pipeline.', 'C.Maintain data quality rules in a separate Databricks notebook that each DLT notebook or file can import as a library.', ""D.Maintain data quality rules in a Delta table outside of this pipeline's target schema, providing the schema name as a pipeline parameter.""]","[""D.Maintain data quality rules in a Delta table outside of this pipeline's target schema, providing the schema name as a pipeline parameter.""]",[],
213,examtopics databricks data engineer professional certification question 213,https://www.examtopics.com/discussions/databricks/view/303320-exam-certified-data-engineer-professional-topic-1-question/,The DevOps team has configured a production workload as a collection of notebooks scheduled to run daily using the Jobs UI. A new data engineering hire is onboarding to the team and has requested access to one of these notebooks to review the production logic.What are the maximum notebook permissions that can be granted to the user without allowing accidental changes to production code or data?,"['A.Can manage', 'B.Can edit', 'C.Can run', 'D.Can read']",['D.Can read'],[],
214,examtopics databricks data engineer professional certification question 214,https://www.examtopics.com/discussions/databricks/view/303322-exam-certified-data-engineer-professional-topic-1-question/,"A table named user_ltv is being used to create a view that will be used by data analysts on various teams. Users in the workspace are configured into groups, which are used for setting up data access using ACLs.The user_ltv table has the following schema:email STRING, age INT, ltv INTThe following view definition is executed:An analyst who is not a member of the marketing group executes the following query:SELECT * FROM email_ltv -Which statement describes the results returned by this query?","['A.Three columns will be returned, but one column will be named ""REDACTED"" and contain only null values.', 'B.Only the email and ltv columns will be returned; the email column will contain all null values.', 'C.The email and ltv columns will be returned with the values in user_ltv.', 'D.Only the email and ltv columns will be returned; the email column will contain the string ""REDACTED"" in each row.']","['D.Only the email and ltv columns will be returned; the email column will contain the string ""REDACTED"" in each row.']",['https://img.examtopics.com/certified-data-engineer-professional/image91.png'],
215,examtopics databricks data engineer professional certification question 215,https://www.examtopics.com/discussions/databricks/view/303323-exam-certified-data-engineer-professional-topic-1-question/,"The data governance team has instituted a requirement that all tables containing Personal Identifiable Information (PII) must be clearly annotated. This includes adding column comments, table comments, and setting the custom table property ""contains_pii"" = true.The following SQL DDL statement is executed to create a new table:Which command allows manual confirmation that these three requirements have been met?","['A.DESCRIBE EXTENDED dev.pii_test', 'B.DESCRIBE DETAIL dev.pii_test', 'C.SHOW TBLPROPERTIES dev.pii_test', 'D.DESCRIBE HISTORY dev.pii_test']",['A.DESCRIBE EXTENDED dev.pii_test'],['https://img.examtopics.com/certified-data-engineer-professional/image92.png'],
216,examtopics databricks data engineer professional certification question 216,https://www.examtopics.com/discussions/databricks/view/303324-exam-certified-data-engineer-professional-topic-1-question/,"The data governance team is reviewing code used for deleting records for compliance with GDPR. They note the following logic is used to delete records from the Delta Lake table named users.Assuming that user_id is a unique identifying key and that delete_requests contains all users that have requested deletion, which statement describes whether successfully executing the above logic guarantees that the records to be deleted are no longer accessible and why?","['A.Yes; Delta Lake ACID guarantees provide assurance that the DELETE command succeeded fully and permanently purged these records.', 'B.No; files containing deleted records may still be accessible with time travel until a VACUUM command is used to remove invalidated data files.', 'C.Yes; the Delta cache immediately updates to reflect the latest data files recorded to disk.', 'D.No; the Delta Lake DELETE command only provides ACID guarantees when combined with the MERGE INTO command.']",['B.No; files containing deleted records may still be accessible with time travel until a VACUUM command is used to remove invalidated data files.'],['https://img.examtopics.com/certified-data-engineer-professional/image93.png'],
217,examtopics databricks data engineer professional certification question 217,https://www.examtopics.com/discussions/databricks/view/303325-exam-certified-data-engineer-professional-topic-1-question/,"The data architect has decided that once data has been ingested from external sources into theDatabricks Lakehouse, table access controls will be leveraged to manage permissions for all production tables and views.The following logic was executed to grant privileges for interactive queries on a production database to the core engineering group.GRANT USAGE ON DATABASE prod TO eng;GRANT SELECT ON DATABASE prod TO eng;Assuming these are the only privileges that have been granted to the eng group and that these users are not workspace administrators, which statement describes their privileges?","['A.Group members are able to create, query, and modify all tables and views in the prod database, but cannot define custom functions.', 'B.Group members are able to list all tables in the prod database but are not able to see the results of any queries on those tables.', 'C.Group members are able to query and modify all tables and views in the prod database, but cannot create new tables or views.', 'D.Group members are able to query all tables and views in the prod database, but cannot create or edit anything in the database.']","['D.Group members are able to query all tables and views in the prod database, but cannot create or edit anything in the database.']",[],
218,examtopics databricks data engineer professional certification question 218,https://www.examtopics.com/discussions/databricks/view/152712-exam-certified-data-engineer-professional-topic-1-question/,"A user wants to use DLT expectations to validate that a derived table report contains all records from the source, included in the table validation_copy.The user attempts and fails to accomplish this by adding an expectation to the report table definition.Which approach would allow using DLT expectations to validate all expected records are present in this table?","['A.Define a temporary table that performs a left outer join on validation_copy and report, and define an expectation that no report key values are null', 'B.Define a SQL UDF that performs a left outer join on two tables, and check if this returns null values for report key values in a DLT expectation for the report table', 'C.Define a view that performs a left outer join on validation_copy and report, and reference this view in DLT expectations for the report table', 'D.Define a function that performs a left outer join on validation_copy and report, and check against the result in a DLT expectation for the report table']","['A.Define a temporary table that performs a left outer join on validation_copy and report, and define an expectation that no report key values are null']",['https://img.examtopics.com/certified-data-engineer-professional/image94.png'],
219,examtopics databricks data engineer professional certification question 219,https://www.examtopics.com/discussions/databricks/view/153569-exam-certified-data-engineer-professional-topic-1-question/,"A user new to Databricks is trying to troubleshoot long execution times for some pipeline logic they are working on. Presently, the user is executing code cell-by-cell, using display() calls to confirm code is producing the logically correct results as new transformations are added to an operation. To get a measure of average time to execute, the user is running each cell multiple times interactively.Which of the following adjustments will get a more accurate measure of how code is likely to perform in production?","['A.The Jobs UI should be leveraged to occasionally run the notebook as a job and track execution time during incremental code development because Photon can only be enabled on clusters launched for scheduled jobs.', 'B.The only way to meaningfully troubleshoot code execution times in development notebooks is to use production-sized data and production-sized clusters with Run All execution.', 'C.Production code development should only be done using an IDE; executing code against a local build of open source Spark and Delta Lake will provide the most accurate benchmarks for how code will perform in production.', 'D.Calling display() forces a job to trigger, while many transformations will only add to the logical query plan; because of caching, repeated execution of the same logic does not provide meaningful results.']","['D.Calling display() forces a job to trigger, while many transformations will only add to the logical query plan; because of caching, repeated execution of the same logic does not provide meaningful results.']",[],
220,examtopics databricks data engineer professional certification question 220,https://www.examtopics.com/discussions/databricks/view/152713-exam-certified-data-engineer-professional-topic-1-question/,Where in the Spark UI can one diagnose a performance problem induced by not leveraging predicate push-down?,"['A.In the Executor’s log file, by grepping for ""predicate push-down""', 'B.In the Stage’s Detail screen, in the Completed Stages table, by noting the size of data read from the Input column', 'C.In the Query Detail screen, by interpreting the Physical Plan', 'D.In the Delta Lake transaction log. by noting the column statistics']","['C.In the Query Detail screen, by interpreting the Physical Plan']",[],
221,examtopics databricks data engineer professional certification question 221,https://www.examtopics.com/discussions/databricks/view/152716-exam-certified-data-engineer-professional-topic-1-question/,"A data engineer needs to capture pipeline settings from an existing setting in the workspace, and use them to create and version a JSON file to create a new pipeline.Which command should the data engineer enter in a web terminal configured with the Databricks CLI?","['A.Use list pipelines to get the specs for all pipelines; get the pipeline spec from the returned results; parse and use this to create a pipeline', 'B.Stop the existing pipeline; use the returned settings in a reset command', 'C.Use the get command to capture the settings for the existing pipeline; remove the pipeline_id and rename the pipeline; use this in a create command', 'D.Use the clone command to create a copy of an existing pipeline; use the get JSON command to get the pipeline definition; save this to git']",['C.Use the get command to capture the settings for the existing pipeline; remove the pipeline_id and rename the pipeline; use this in a create command'],[],
222,examtopics databricks data engineer professional certification question 222,https://www.examtopics.com/discussions/databricks/view/155084-exam-certified-data-engineer-professional-topic-1-question/,Which Python variable contains a list of directories to be searched when trying to locate required modules?,"['A.importlib.resource_path', 'B.sys.path', 'C.os.path', 'D.pypi.path']",['B.sys.path'],[],
223,examtopics databricks data engineer professional certification question 223,,,[],[],[],Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
224,examtopics databricks data engineer professional certification question 224,,,[],[],[],Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
225,examtopics databricks data engineer professional certification question 225,https://www.examtopics.com/discussions/databricks/view/152622-exam-certified-data-engineer-professional-topic-1-question/,Which REST API call can be used to review the notebooks configured to run as tasks in a multi-task job?,"['A./jobs/runs/list', 'B./jobs/list', 'C./jobs/runs/get', 'D./jobs/get']",['D./jobs/get'],[],
226,examtopics databricks data engineer professional certification question 226,https://www.examtopics.com/discussions/databricks/view/152717-exam-certified-data-engineer-professional-topic-1-question/,A Data Engineer wants to run unit tests using common Python testing frameworks on Python functions defined across several Databricks notebooks currently used in production.How can the data engineer run unit tests against functions that work with data in production?,"['A.Define and import unit test functions from a separate Databricks notebook', 'B.Define and unit test functions using Files in Repos', 'C.Run unit tests against non-production data that closely mirrors production', 'D.Define unit tests and functions within the same notebook']",['B.Define and unit test functions using Files in Repos'],[],
227,examtopics databricks data engineer professional certification question 227,https://www.examtopics.com/discussions/databricks/view/152742-exam-certified-data-engineer-professional-topic-1-question/,"A data engineer wants to refactor the following DLT code, which includes multiple table definitions with very similar code.In an attempt to programmatically create these tables using a parameterized table definition, the data engineer writes the following code.The pipeline runs an update with this refactored code, but generates a different DAG showing incorrect configuration values for these tables.How can the data engineer fix this?","['A.Wrap the for loop inside another table definition, using generalized names and properties to replace with those from the inner table definition.', 'B.Convert the list of configuration values to a dictionary of table settings, using table names as keys.', 'C.Move the table definition into a separate function, and make calls to this function using different input parameters inside the for loop.', 'D.Load the configuration values for these tables from a separate file, located at a path provided by a pipeline parameter.']","['C.Move the table definition into a separate function, and make calls to this function using different input parameters inside the for loop.']","['https://img.examtopics.com/certified-data-engineer-professional/image95.png', 'https://img.examtopics.com/certified-data-engineer-professional/image96.png']",
228,examtopics databricks data engineer professional certification question 228,https://www.examtopics.com/discussions/databricks/view/316704-exam-certified-data-engineer-professional-topic-1-question/,A data engineer has created a 'transactions' Delta table on Databricks that should be used by the analytics team. The analytics team wants to use the table with another tool which requires Apache Iceberg format.What should the data engineer do?,"['A.Require the analytics team to use a tool which supports Delta table.', ""B.Create an Iceberg copy of the 'transactions' Delta table which can be used by the analytics team."", ""C.Convert the 'transactions' Delta to Iceberg and enable uniform so that the table can be read as a Delta table."", ""D.Enable uniform on the transactions table to 'iceberg' so that the table can be read as an Iceberg table.""]","[""D.Enable uniform on the transactions table to 'iceberg' so that the table can be read as an Iceberg table.""]",[],
229,examtopics databricks data engineer professional certification question 229,https://www.examtopics.com/discussions/databricks/view/316893-exam-certified-data-engineer-professional-topic-1-question/,"A junior data engineer is working to implement logic for a Lakehouse table named silver_device_recordings. The source data contains 100 unique fields in a highly nested JSON structure.The silver_device_recordings table will be used downstream for highly selective joins on a number of fields, and will also be leveraged by the machine learning team to filter on a handful of relevant fields. In total, 15 fields have been identified that will often be used for filter and join logic.The data engineer is trying to determine the best approach for dealing with these nested fields before declaring the table schema.Which of the following accurately presents information about Delta Lake and Databricks that may impact their decision-making process?","['A.Because Delta Lake uses Parquet for data storage, Dremel encoding information for nesting can be directly referenced by the Delta transaction log.', 'B.Schema inference and evolution on Databricks ensure that inferred types will always accurately match the data types used by downstream systems.', 'C.The Tungsten encoding used by Databricks is optimized for storing string data; newly-added native support for querying JSON strings means that string types are always most efficient.', 'D.By default, Delta Lake collects statistics on the first 32 columns in a table; these statistics are leveraged for data skipping when executing selective queries.']","['D.By default, Delta Lake collects statistics on the first 32 columns in a table; these statistics are leveraged for data skipping when executing selective queries.']",[],
230,examtopics databricks data engineer professional certification question 230,https://www.examtopics.com/discussions/databricks/view/316892-exam-certified-data-engineer-professional-topic-1-question/,"A platform engineer is creating catalogs and schemas for the development team to use.The engineer has created an initial catalog, Catalog_A, and initial schema, Schema_A. The engineer has also granted USE CATALOG, USE SCHEMA, and CREATE TABLE to the development team so that the engineer can begin populating the schema with new tables.Despite being owner of the catalog and schema, the engineer noticed that they do not have access to the underlying tables in Schema_A.What explains the engineer's lack of access to the underlying tables?","['A.The owner of the schema does not automatically have permission to tables within the schema, but can grant them to themselves at any point.', ""B.Users granted with USE CATALOG can modify the owner's permissions to downstream tables."", 'C.Permissions explicitly given by the table creator are the only way the Platform Engineer could access the underlying tables in their schema.', 'D.The platform engineer needs to execute a REFRESH statement as the table permissions did not automatically update for owners.']","['A.The owner of the schema does not automatically have permission to tables within the schema, but can grant them to themselves at any point.']",[],
231,examtopics databricks data engineer professional certification question 231,https://www.examtopics.com/discussions/databricks/view/316705-exam-certified-data-engineer-professional-topic-1-question/,A data engineer has created a new cluster using shared access mode with default configurations. The data engineer needs to allow the development team access to view the driver logs if needed.What are the minimal cluster permissions that allow the development team to accomplish this?,"['A.CAN VIEW', 'B.CAN RESTART', 'C.CAN ATTACH TO', 'D.CAN MANAGE']",['A.CAN VIEW'],[],
232,examtopics databricks data engineer professional certification question 232,https://www.examtopics.com/discussions/databricks/view/316867-exam-certified-data-engineer-professional-topic-1-question/,A data engineer wants to create a cluster using the Databricks CLI for a big ETL pipeline. The cluster should have five workers and one driver of type i3.xlarge and should use the '14.3.x-scala2.12' runtime.Which command should the data engineer use?,"['A.databricks compute add 14.3.x-scala2.12 --num-workers 5 --node-type-id i3.xlarge --cluster-name Data Engineer_cluster', 'B.databricks clusters create 14.3.x-scala2.12 --num-workers 5 --node-type-id i3.xlarge --cluster-name Data Engineer_cluster', 'C.databricks compute create 14.3.x-scala2.12 --num-workers 5 --node-type-id i3.xlarge --cluster-name Data Engineer_cluster', 'D.databricks clusters add 14.3.x-scala2.12 --num-workers 5 --node-type-id i3.xlarge --cluster-name Data Engineer_cluster']",['B.databricks clusters create 14.3.x-scala2.12 --num-workers 5 --node-type-id i3.xlarge --cluster-name Data Engineer_cluster'],[],
233,examtopics databricks data engineer professional certification question 233,https://www.examtopics.com/discussions/databricks/view/316707-exam-certified-data-engineer-professional-topic-1-question/,"A 'transactions' table has been liquid clustered on the columns 'product_id’, ’user_id' and 'event_date'.Which operation lacks support for cluster on write?","['A.CTAS and RTAS statements', ""B.spark.writeStream.format(’delta').mode(’append’)"", ""C.spark.write.format('delta’).mode('append')"", 'D.INSERT INTO operations']","[""B.spark.writeStream.format(’delta').mode(’append’)""]",[],
234,examtopics databricks data engineer professional certification question 234,https://www.examtopics.com/discussions/databricks/view/316713-exam-certified-data-engineer-professional-topic-1-question/,"The data governance team has instituted a requirement that the ""user"" table containing Personal Identifiable Information (PII) must have the appropriate masking on the SSN column. This means that anyone outside of the HRAdminGroup should see masked social security numbers as ***-**-****.The team created a masking function:What does the data governance team need to do next to achieve this goal?","['A.CREATE TABLE users -(name STRING);ALTER TABLE users CREATE COLUMN ssn CREATE MASK ssn_mask;', ""B.CREATE TABLE users -(name STRING, int STRING);ALTER TABLE users ALTER COLUMN ssn CREATE MASK if is_member('HRAdminGroup');"", 'C.CREATE TABLE users -(name STRING, ssn INT MASKED ssn_mask);', 'D.CREATE TABLE users -(name STRING, ssn STRING);ALTER TABLE users ALTER COLUMN ssn SET MASK ssn_mask;']","['D.CREATE TABLE users -(name STRING, ssn STRING);ALTER TABLE users ALTER COLUMN ssn SET MASK ssn_mask;']",['https://img.examtopics.com/certified-data-engineer-professional/image97.png'],
235,examtopics databricks data engineer professional certification question 235,https://www.examtopics.com/discussions/databricks/view/316708-exam-certified-data-engineer-professional-topic-1-question/,A data engineer needs to create an application that will collect information about the latest job run including the repair history.How should the data engineer format the request?,"['A.Call/api/2.1/jobs/runs/list with the run_id and include_history parameters', 'B.Call/api/2.1/jobs/runs/get with the run_id and include_history parameters', 'C.Call/api/2.1/jobs/runs/get with the job_id and include_history parameters', 'D.Call/api/2.1/jobs/runs/list with the job_id and include_history parameters']",['B.Call/api/2.1/jobs/runs/get with the run_id and include_history parameters'],[],
236,examtopics databricks data engineer professional certification question 236,https://www.examtopics.com/discussions/databricks/view/316714-exam-certified-data-engineer-professional-topic-1-question/,"A data engineer is working in an interactive notebook with many transformations before outputting the result from display(df.collect() ). The notebook includes wide transformations and a cross join.The data engineer is getting the following error: ""The spark driver has stopped unexpectedly and is restarting. Your notebook will be automatically reattached.""Which action should the data engineer take?","['A.Run the notebook on a single node cluster to keep driver from falling.', 'B.Rewrite their code to avoid putting memory pressure on the driver node.', 'C.Check into the Spark UI to see how many jobs are assigned to each stage as they are employing fewer executors.', 'D.Look at the compute metrics UI to see if the executors have higher than 90% memory utilization.']",['B.Rewrite their code to avoid putting memory pressure on the driver node.'],[],
237,examtopics databricks data engineer professional certification question 237,https://www.examtopics.com/discussions/databricks/view/316715-exam-certified-data-engineer-professional-topic-1-question/,An analytics team wants run an experiment in the short term on the customer transaction Delta table (with 20 billions records) created by the data engineering team in Databricks SQL.Which strategy should the data engineering team use to ensure minimal downtime and no impact on the ongoing ETL processes?,"['A.Deep clone the table for the analytics team.', 'B.Create a new table for the analytics team using a CTAS statement.', 'C.Shallow clone the table for the analytics team.', 'D.Give access to the table for the analytics team.']",['C.Shallow clone the table for the analytics team.'],[],
238,examtopics databricks data engineer professional certification question 238,https://www.examtopics.com/discussions/databricks/view/316716-exam-certified-data-engineer-professional-topic-1-question/,"A data team is working to optimize an existing large, fast-growing table 'orders' with high cardinality columns, which experiences significant data skew and requires frequent concurrent writes. The team notice that the columns 'user_id', 'event_timestamp' and 'product_id' are heavily used in analytical queries and filters, although those keys may be subject to change in the future due to different business requirements.Which partitioning strategy should the team choose to optimize the table for immediate data skipping, incremental management over time, and flexibility?","['A.Partition the table with: ALTER TABLE orders PARTITION BY user_id, product_id, event_timestamp', 'B.Use z-order after partitiing the table: OPTIMIZE orders ZORDER BY (user_id, product_id) WHERE event_timestamp = current date () - 1 DAY', 'C.Cluster the table with: ALTER TABLE orders CLUSTER BY user_id, product_id, event_timestamp', 'D.Z-order the table with OPTIMIZE orders ZORDER BY (user_id, product_id, event_timestamp)']","['D.Z-order the table with OPTIMIZE orders ZORDER BY (user_id, product_id, event_timestamp)']",[],
239,examtopics databricks data engineer professional certification question 239,,,[],[],[],Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
240,examtopics databricks data engineer professional certification question 240,,,[],[],[],Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
241,examtopics databricks data engineer professional certification question 241,,,[],[],[],Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
242,examtopics databricks data engineer professional certification question 242,,,[],[],[],Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
243,examtopics databricks data engineer professional certification question 243,,,[],[],[],Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
244,examtopics databricks data engineer professional certification question 244,,,[],[],[],Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
245,examtopics databricks data engineer professional certification question 245,,,[],[],[],Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
246,examtopics databricks data engineer professional certification question 246,,,[],[],[],Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
247,examtopics databricks data engineer professional certification question 247,,,[],[],[],Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
248,examtopics databricks data engineer professional certification question 248,,,[],[],[],Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
249,examtopics databricks data engineer professional certification question 249,,,[],[],[],Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
250,examtopics databricks data engineer professional certification question 250,,,[],[],[],Invalid URL 'None': No scheme supplied. Perhaps you meant https://None?
